{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeuhzp+1zipXNYV6yf1ObO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/PyTorch-Learning-Repository/blob/main/08_Generative_Models_(GANs%2C_VAEs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j3nV-0Ve1b5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **8.1. Introduction to Generative Models**\n",
        "- Generative models learn the joint probability distribution of the input data, allowing them to generate new samples from the learned distribution.\n",
        "- They can be broadly classified into two categories:\n",
        "  - **Implicit Models**: Learn to generate data directly from the data distribution (e.g., GANs).\n",
        "  - **Explicit Models**: Learn to model the data distribution directly (e.g., VAEs).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "42E2pL3y8NyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **8.2. Generative Adversarial Networks (GANs)**\n",
        "- **GANs** are a class of generative models introduced by Ian Goodfellow and his colleagues in 2014. They consist of two neural networks: the **generator** and the **discriminator**.\n",
        "  \n",
        "**Key Components of GANs**:\n",
        "1. **Generator**: This network generates new data samples from random noise. Its goal is to produce samples that are indistinguishable from real data.\n",
        "2. **Discriminator**: This network evaluates the generated samples and determines whether they are real (from the dataset) or fake (generated by the generator).\n",
        "3. **Adversarial Training**: The generator and discriminator are trained simultaneously in a game-theoretic setup where the generator tries to fool the discriminator while the discriminator tries to correctly identify real and fake samples.\n",
        "\n",
        "---\n",
        "\n",
        "**8.2.1. GAN Training Process**\n",
        "- The training process involves:\n",
        "  1. Sampling random noise and generating a fake sample using the generator.\n",
        "  2. Sampling real data from the dataset.\n",
        "  3. Training the discriminator on both real and fake samples to improve its ability to distinguish between them.\n",
        "  4. Training the generator to improve its ability to create realistic samples that can fool the discriminator.\n",
        "  \n",
        "This process continues until the generator produces samples that are indistinguishable from real data.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1No9b3ac8O0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **8.3. Building a GAN**\n",
        "We will now implement a simple GAN to generate images from the MNIST dataset, which consists of handwritten digits.\n",
        "\n",
        "---\n",
        "\n",
        "**8.3.1. Example: Building and Training a GAN**"
      ],
      "metadata": {
        "id": "EpSpCkJJ8O42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn  # Neural network modules\n",
        "import torch.optim as optim  # Optimization algorithms like Adam\n",
        "import torchvision  # PyTorch's computer vision package\n",
        "import torchvision.transforms as transforms  # Tools for transforming data\n",
        "import matplotlib.pyplot as plt  # Used for visualizing generated images\n",
        "\n",
        "# Step 1: Define the generator network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100, 256),  # Input is a noise vector of size 100 (latent space)\n",
        "            nn.ReLU(),  # Activation function: ReLU\n",
        "            nn.Linear(256, 512),  # Hidden layer with 512 units\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1024),  # Hidden layer with 1024 units\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 28 * 28),  # Output size is 28x28 (MNIST image flattened)\n",
        "            nn.Tanh()  # Tanh activation scales output to range [-1, 1], which matches the normalized input data\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z).view(-1, 1, 28, 28)  # Reshape the output to match the image size (batch_size, 1, 28, 28)\n"
      ],
      "metadata": {
        "id": "ljWetlMh8O42"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Define the discriminator network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28 * 28, 1024),  # Input is a flattened 28x28 image\n",
        "            nn.LeakyReLU(0.2),  # Leaky ReLU activation with slope 0.2 for negative values\n",
        "            nn.Linear(1024, 512),  # Hidden layer with 512 units\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),  # Hidden layer with 256 units\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),  # Output layer produces a single score (real or fake)\n",
        "            nn.Sigmoid()  # Sigmoid activation to output probability (between 0 and 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x.view(-1, 28 * 28))  # Flatten input image for fully connected layers\n"
      ],
      "metadata": {
        "id": "oLTm2rOZp1cc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Initialize the generator and discriminator networks\n",
        "generator = Generator()  # Instantiate the generator\n",
        "discriminator = Discriminator()  # Instantiate the discriminator\n"
      ],
      "metadata": {
        "id": "GU7q1ibMp1Qj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Define loss function and optimizers\n",
        "criterion = nn.BCELoss()  # Binary cross-entropy loss for classification (real/fake)\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Optimizer for generator\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Optimizer for discriminator\n"
      ],
      "metadata": {
        "id": "YLeEdRaqp1NA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Load the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to tensors\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize images to range [-1, 1] to match the output of the generator (Tanh)\n",
        "])\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)  # DataLoader for batching and shuffling\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzWot2nSp1HX",
        "outputId": "3abb2df0-7e6e-4902-f9d9-093768ca526f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 53032137.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1845342.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 14635413.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3715726.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Train the GAN\n",
        "num_epochs = 5  # Number of epochs for training\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, _) in enumerate(train_loader):  # Loop over batches of real images\n",
        "\n",
        "        # Create labels for real and fake images\n",
        "        real_labels = torch.ones(real_images.size(0), 1)  # Real images have label 1\n",
        "        fake_labels = torch.zeros(real_images.size(0), 1)  # Fake images have label 0\n",
        "\n",
        "        # Step 6.1: Train the discriminator\n",
        "        optimizer_D.zero_grad()  # Zero the gradients of the discriminator\n",
        "\n",
        "        # Forward pass for real images\n",
        "        outputs = discriminator(real_images)  # Get discriminator's prediction on real images\n",
        "        d_loss_real = criterion(outputs, real_labels)  # Calculate loss for real images\n",
        "        d_loss_real.backward()  # Backpropagate the loss for real images\n",
        "\n",
        "        # Generate fake images\n",
        "        noise = torch.randn(real_images.size(0), 100)  # Generate noise vectors (latent space) of size 100\n",
        "        fake_images = generator(noise)  # Use generator to create fake images from noise\n",
        "\n",
        "        # Forward pass for fake images\n",
        "        outputs = discriminator(fake_images.detach())  # Get discriminator's prediction on fake images (detach to avoid updating generator during this pass)\n",
        "        d_loss_fake = criterion(outputs, fake_labels)  # Calculate loss for fake images\n",
        "        d_loss_fake.backward()  # Backpropagate the loss for fake images\n",
        "\n",
        "        optimizer_D.step()  # Update the discriminator's weights\n",
        "\n",
        "        # Step 6.2: Train the generator\n",
        "        optimizer_G.zero_grad()  # Zero the gradients of the generator\n",
        "\n",
        "        # Forward pass for fake images (generator wants discriminator to classify them as real)\n",
        "        outputs = discriminator(fake_images)  # Get discriminator's prediction for fake images\n",
        "        g_loss = criterion(outputs, real_labels)  # Generator's goal is to fool the discriminator, so we use real labels for fake images\n",
        "        g_loss.backward()  # Backpropagate the loss for the generator\n",
        "\n",
        "        optimizer_G.step()  # Update the generator's weights\n",
        "\n",
        "    # Print the loss after each epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], D Loss: {d_loss_real.item() + d_loss_fake.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
        "\n",
        "    # Step 7: Generate and save images at intervals\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        with torch.no_grad():  # Disable gradient calculation for generating images\n",
        "            sample_noise = torch.randn(16, 100)  # Generate a batch of 16 noise vectors\n",
        "            generated_images = generator(sample_noise)  # Generate images from the noise\n",
        "            generated_images = generated_images.view(-1, 1, 28, 28)  # Reshape the images to 28x28 size\n",
        "\n",
        "            # Create a grid of generated images\n",
        "            grid = torchvision.utils.make_grid(generated_images, nrow=4, normalize=True)\n",
        "            plt.imshow(grid.permute(1, 2, 0).numpy())  # Permute dimensions to match the format for plotting (HWC)\n",
        "            plt.title(f'Epoch {epoch + 1}')\n",
        "            plt.axis('off')  # Remove axis labels\n",
        "            plt.show()  # Display the generated images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9Wuthyrp1KV",
        "outputId": "7f77e865-3934-4df6-dc03-e346e34455fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], D Loss: 0.3720, G Loss: 2.5657\n",
            "Epoch [2/5], D Loss: 0.4795, G Loss: 2.1411\n",
            "Epoch [3/5], D Loss: 0.8254, G Loss: 2.4208\n",
            "Epoch [4/5], D Loss: 0.6313, G Loss: 1.7668\n",
            "Epoch [5/5], D Loss: 0.9390, G Loss: 1.6317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "- **Generator Network**: A simple feedforward neural network that takes a noise vector and generates a 28x28 image.\n",
        "- **Discriminator Network**: A neural network that takes an image (real or fake) and outputs a probability indicating whether the image is real.\n",
        "- **Training Loop**:\n",
        "  - The discriminator is trained to differentiate between real and fake images.\n",
        "  - The generator is trained to produce images that can fool the discriminator.\n",
        "  - Losses are calculated using Binary Cross Entropy, and the networks are optimized using the Adam optimizer.\n",
        "- **Sample Generation**: Every 10 epochs, the model generates and displays sample images produced by the generator.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "uoVblLfT8O8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **8.4. Variational Autoencoders (VAEs)**\n",
        "- **Variational Autoencoders (VAEs)** are another class of generative models that learn to encode input data into a latent space and then decode it back to the original space.\n",
        "- VAEs are designed to generate new data points by sampling from the learned latent space, capturing the distribution of the input data.\n",
        "\n",
        "**Key Components of VAEs**:\n",
        "1. **Encoder**: Maps input data to a latent representation. It outputs the parameters of the latent distribution (mean and variance).\n",
        "2. **Latent Space**: A compressed representation of the input data, which captures the essential features of the input.\n",
        "3. **Decoder**: Reconstructs data from the latent representation. It generates new samples by sampling from the latent space and passing it through the decoder.\n",
        "\n",
        "---\n",
        "\n",
        "**8.4.1. VAE Training Process**\n",
        "- The training process for a VAE involves two main losses:\n",
        "  1. **Reconstruction Loss**: Measures how well the decoder reconstructs the input from the latent space. This is often calculated using mean squared error or binary cross-entropy.\n",
        "  2. **Kullback-Leibler Divergence (KL Divergence)**: Measures how much the learned latent distribution deviates from a standard normal distribution. This encourages the model to learn a more structured latent space.\n",
        "\n",
        "The overall loss function combines both reconstruction loss and KL divergence.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_UUUAYUT8O_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **8.6. Applications of GANs and VAEs**\n",
        "- **Image Generation**: GANs are widely used to generate realistic images, such as faces or artworks, and can create high-quality images that are indistinguishable from real ones.\n",
        "- **Data Augmentation**: Both GANs and VAEs can be used to augment datasets by generating additional synthetic data, which is particularly useful in scenarios with limited data.\n",
        "- **Text and Audio Generation**: VAEs have been applied to generate sequences in NLP and to synthesize audio signals, producing realistic speech or music.\n",
        "- **Style Transfer**: GANs and VAEs are employed in tasks like style transfer, where the model learns to apply the style of one image to the content of another.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ichC1eAi8PJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **8.7. Observations on Generative Models**\n",
        "- **GANs**:\n",
        "  - **Training Stability**: Training GANs can be tricky; if the generator becomes too good too quickly, the discriminator may not learn effectively. Techniques like gradient penalty and historical averaging have been proposed to stabilize training.\n",
        "  - **Mode Collapse**: A common problem where the generator learns to produce a limited variety of outputs. Various techniques, such as mini-batch discrimination, can mitigate this issue.\n",
        "\n",
        "- **VAEs**:\n",
        "  - **Latent Space Representation**: VAEs provide a well-structured latent space, which allows for meaningful interpolation between data points.\n",
        "  - **Continuous Generation**: The continuous nature of the latent space in VAEs enables smooth transitions between generated samples, making them suitable for tasks requiring variability.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "c6SBy-6b8PNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Continuity to the Next Section\n",
        "- In the next section, we will explore **Reinforcement Learning**, a powerful paradigm where agents learn to make decisions by interacting with an environment. We will cover key concepts, algorithms, and applications in various domains.\n",
        "\n",
        "This section provided an in-depth overview of generative models, including GANs and VAEs. We discussed their architectures, training processes, and applications in image and data generation. These models represent a significant advancement in machine learning, enabling the creation of realistic and complex data distributions."
      ],
      "metadata": {
        "id": "oSJ3t8fK8PQ9"
      }
    }
  ]
}