{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyNR/BLrgNuNaMF5gYhfnz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/PyTorch-Learning-Repository/blob/main/03_Dataset_and_DataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wU-QeRLBFtoP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3UyoIgkn8FsJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **3.1. Introduction to Datasets and DataLoader**\n",
        "- In PyTorch, the `torch.utils.data.Dataset` class represents a dataset, while the `DataLoader` class helps to load data in mini-batches, shuffle data, and load data in parallel using multiple workers.\n",
        "- **Dataset**: A dataset is a collection of data samples that PyTorch models can learn from. PyTorch provides an easy-to-use interface to load custom datasets and built-in datasets.\n",
        "- **DataLoader**: It is used to efficiently iterate through datasets during training. It handles batching, shuffling, and parallel data loading using multiple CPU cores.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "42E2pL3y8NyW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SCBy22qK8N_n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **3.2. Built-in Datasets in PyTorch**\n",
        "- PyTorch provides access to several standard datasets through `torchvision.datasets`, including MNIST, CIFAR-10, and ImageNet.\n",
        "- These datasets can be downloaded and automatically preprocessed using transformations such as normalization, resizing, and data augmentation.\n",
        "\n",
        "---\n",
        "\n",
        "**3.2.1. Loading the MNIST Dataset (Example)**\n",
        "- MNIST is a popular dataset consisting of handwritten digits, with 60,000 training images and 10,000 test images. Each image is 28x28 pixels and contains a grayscale digit (0-9).\n",
        "\n",
        "**Demonstration: Loading and Visualizing MNIST Dataset**"
      ],
      "metadata": {
        "id": "1No9b3ac8O0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define transformations to apply to the MNIST images:\n",
        "# 1. Convert images to PyTorch tensors using `transforms.ToTensor()`.\n",
        "# 2. Normalize the image data to have values between -1 and 1 using `transforms.Normalize()`.\n",
        "#    - The MNIST images are grayscale, so we normalize using mean (0.5,) and std (0.5,).\n",
        "#    - This helps stabilize training by keeping input values consistent.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors (range [0, 1])\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Normalize to the range [-1, 1]\n",
        "])\n",
        "\n",
        "# Download and load the training data for MNIST\n",
        "# - `root='./data'`: The directory where the dataset will be downloaded and stored.\n",
        "# - `train=True`: Download the training data.\n",
        "# - `download=True`: Download the dataset if it's not already present in the `root` directory.\n",
        "# - `transform=transform`: Apply the transformations defined above to each image.\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Download and load the test data for MNIST\n",
        "# - Similar to the train dataset, but with `train=False` to get the test data.\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Use DataLoader to load the dataset in mini-batches\n",
        "# - `batch_size=64`: Load the data in batches of 64 images for training.\n",
        "# - `shuffle=True`: Shuffle the training data at each epoch to randomize the order of the samples.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Load the test data in mini-batches\n",
        "# - `batch_size=64`: Load the test data in batches of 64.\n",
        "# - `shuffle=False`: No need to shuffle test data.\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Visualizing a sample image from the training dataset\n",
        "# - We use `iter(train_loader)` to get an iterator over the training data.\n",
        "# - `next(examples)` gives us the next batch of data (both images and labels).\n",
        "examples = iter(train_loader)\n",
        "example_data, example_targets = next(examples) # Changed from examples.next() to next(examples)\n",
        "\n",
        "# Plot the first image in the current batch of training images\n",
        "# - `example_data[0][0]` refers to the first image in the batch.\n",
        "#   - The first index `[0]` selects the first image in the batch.\n",
        "#   - The second index `[0]` selects the first channel (since MNIST images are grayscale, there's only one channel).\n",
        "plt.imshow(example_data[0][0], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.imshow(example_data[1][0], cmap='gray')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rlqltPd18O0o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c8023c9-a34c-47bf-cea1-ae07e7b527f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 54553354.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 2157358.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 12662664.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1007)>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3836191.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAb8ElEQVR4nO3dfWxV9R3H8c/l6Qra3q7W9vZKiwUUFh5qxqBrUNTR0FZjAInDhz/QGA2sGBEftm4qOt06WbY5DcMlc4BRfCAbMI3rJtWWzbUQEMZ0s6GkW2ugZZL03lJoy+hvfxDvuELBc7m33z68X8kv4Z5zvj1ffh7vh3PPuac+55wTAAB9bJh1AwCAoYkAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgIkR1g18UU9Pjw4ePKiUlBT5fD7rdgAAHjnn1N7erlAopGHDej/P6XcBdPDgQeXk5Fi3AQC4QM3NzRo7dmyv6/vdR3ApKSnWLQAAEuB87+dJC6A1a9boiiuu0EUXXaSCggLt3LnzS9XxsRsADA7nez9PSgC98cYbWrlypVatWqUPP/xQ+fn5Ki4u1uHDh5OxOwDAQOSSYNasWa6srCz6+uTJky4UCrmKiorz1obDYSeJwWAwGAN8hMPhc77fJ/wMqLu7W7t371ZRUVF02bBhw1RUVKTa2toztu/q6lIkEokZAIDBL+EB9Nlnn+nkyZPKysqKWZ6VlaWWlpYztq+oqFAgEIgO7oADgKHB/C648vJyhcPh6GhubrZuCQDQBxL+PaCMjAwNHz5cra2tMctbW1sVDAbP2N7v98vv9ye6DQBAP5fwM6BRo0ZpxowZqqqqii7r6elRVVWVCgsLE707AMAAlZQnIaxcuVJLlizR17/+dc2aNUvPPfecOjo6dPfddydjdwCAASgpAbR48WL95z//0RNPPKGWlhZdffXVqqysPOPGBADA0OVzzjnrJk4XiUQUCASs2wAAXKBwOKzU1NRe15vfBQcAGJoIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmBhh3QAw0I0ZM8ZzzeLFiz3XfPe73/Vck5OT47lGkjZu3Oi55te//rXnmrq6Os81GDw4AwIAmCCAAAAmEh5ATz75pHw+X8yYPHlyoncDABjgknINaMqUKdq2bdv/dzKCS00AgFhJSYYRI0YoGAwm40cDAAaJpFwD2r9/v0KhkMaPH68777xTTU1NvW7b1dWlSCQSMwAAg1/CA6igoEDr169XZWWl1q5dq8bGRl177bVqb28/6/YVFRUKBALREe9towCAgSXhAVRaWqpbb71V06dPV3Fxsd555x21tbXpzTffPOv25eXlCofD0dHc3JzolgAA/VDS7w5IS0vTVVddpYaGhrOu9/v98vv9yW4DANDPJP17QEePHtWBAweUnZ2d7F0BAAaQhAfQww8/rJqaGv3rX//SX//6Vy1cuFDDhw/X7bffnuhdAQAGsIR/BPfpp5/q9ttv15EjR3TZZZfpmmuuUV1dnS677LJE7woAMID5nHPOuonTRSIRBQIB6zYwRMVzPXLDhg2ea2699VbPNT6fz3NNX/7v3d3d7bnmhhtu8FzDA0wHjnA4rNTU1F7X8yw4AIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJngYKQalcz0A8Vz++Mc/eq6ZNWtWXPvy6k9/+pPnmh07dsS1r8cffzyuOq+ampo81/ztb3/zXBPv29zOnTs911RUVMS1r8GIh5ECAPolAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAICJEdYNAMkwe/bsuOr66snWN910k+ea6upqzzXd3d2ea+IVzxO0c3NzPdeMGzfOc83HH3/suUaSHnvssbjq8OVwBgQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEDyPFoOT3+/tsX21tbZ5rGhoaPNd0dnZ6ronXnj17+mxfXlVUVHiueeaZZ+La1/Hjx+Oqw5fDGRAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATPIwUg9L+/fvjqvP5fJ5rWltbPdfE8zDSeEyYMCGuul/84heea+KZu9/+9reea77//e97rkH/xBkQAMAEAQQAMOE5gLZv366bb75ZoVBIPp9PW7ZsiVnvnNMTTzyh7OxsjR49WkVFRXF/HAIAGLw8B1BHR4fy8/O1Zs2as65fvXq1nn/+eb344ovasWOHLr74YhUXF/fpL9MCAPR/nm9CKC0tVWlp6VnXOef03HPP6bHHHtP8+fMlSS+//LKysrK0ZcsW3XbbbRfWLQBg0EjoNaDGxka1tLSoqKgouiwQCKigoEC1tbVnrenq6lIkEokZAIDBL6EB1NLSIknKysqKWZ6VlRVd90UVFRUKBALRkZOTk8iWAAD9lPldcOXl5QqHw9HR3Nxs3RIAoA8kNICCwaCkM7+Y19raGl33RX6/X6mpqTEDADD4JTSA8vLyFAwGVVVVFV0WiUS0Y8cOFRYWJnJXAIABzvNdcEePHo15jEhjY6P27t2r9PR05ebmasWKFXrmmWd05ZVXKi8vT48//rhCoZAWLFiQyL4BAAOc5wDatWuXbrjhhujrlStXSpKWLFmi9evX69FHH1VHR4fuu+8+tbW16ZprrlFlZaUuuuiixHUNABjwfM45Z93E6SKRiAKBgHUbGOAeffTRuOoqKio813zyySeea6ZMmeK5Jp4Hi77zzjueayQpNzfXc82zzz7bJzXHjx/3XAMb4XD4nNf1ze+CAwAMTQQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAE55/HQMwEGzbti2uuniehp2SkuK55uqrr/Zcs2XLFs81OTk5nmsk6YUXXvBc8+STT8a1LwxdnAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwcNIMSg1NjbGVXf06FHPNZdffrnnmg8++MBzzejRoz3XPP30055rJOmHP/xhXHWAF5wBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMOFzzjnrJk4XiUQUCASs28AQtXr1as81Dz30UBI6OdPf//53zzWFhYVx7ev48eNx1QGnC4fDSk1N7XU9Z0AAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMjLBuAEiGlJSUuOq+9a1vea7x+Xxx7cur/Px8zzV33HFHXPt66aWX4qoDvOAMCABgggACAJjwHEDbt2/XzTffrFAoJJ/Ppy1btsSsv+uuu+Tz+WJGSUlJovoFAAwSngOoo6ND+fn5WrNmTa/blJSU6NChQ9Hx2muvXVCTAIDBx/NNCKWlpSotLT3nNn6/X8FgMO6mAACDX1KuAVVXVyszM1OTJk3SsmXLdOTIkV637erqUiQSiRkAgMEv4QFUUlKil19+WVVVVXr22WdVU1Oj0tJSnTx58qzbV1RUKBAIREdOTk6iWwIA9EMJ/x7QbbfdFv3ztGnTNH36dE2YMEHV1dWaO3fuGduXl5dr5cqV0deRSIQQAoAhIOm3YY8fP14ZGRlqaGg463q/36/U1NSYAQAY/JIeQJ9++qmOHDmi7OzsZO8KADCAeP4I7ujRozFnM42Njdq7d6/S09OVnp6up556SosWLVIwGNSBAwf06KOPauLEiSouLk5o4wCAgc1zAO3atUs33HBD9PXn12+WLFmitWvXat++fdqwYYPa2toUCoU0b948Pf300/L7/YnrGgAw4Pmcc866idNFIhEFAgHrNjDAnf6PJC+2bdvmuebgwYOea3bu3Om5ZuHChZ5rKisrPddI0oIFCzzXdHd3x7UvDF7hcPic1/V5FhwAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwARPw0a/N3nyZM81mzZtimtfaWlpnmtKSko813z88ceeaxobGz3X5Obmeq6R4nvy9u9///u49oXBi6dhAwD6JQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZGWDeAoSWeh31u2LDBc82kSZM810jS/PnzPdfE82DReFRVVXmuufvuu5PQCZAYnAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwcNIEbd4Hiz605/+1HPNzJkzPdd88MEHnmsk6Q9/+ENcdX3hpptu8lzT2dkZ176ampriqgO84AwIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACR5GiriVl5d7rrn77rs912zevNlzzaJFizzXxGvMmDGeazZs2OC5JhgMeq6JZ+4kae/evXHVAV5wBgQAMEEAAQBMeAqgiooKzZw5UykpKcrMzNSCBQtUX18fs01nZ6fKysp06aWX6pJLLtGiRYvU2tqa0KYBAAOfpwCqqalRWVmZ6urq9O677+rEiROaN2+eOjo6ots8+OCDeuutt7Rp0ybV1NTo4MGDuuWWWxLeOABgYPN0E0JlZWXM6/Xr1yszM1O7d+/WnDlzFA6H9dJLL2njxo365je/KUlat26dvvrVr6qurk7f+MY3Etc5AGBAu6BrQOFwWJKUnp4uSdq9e7dOnDihoqKi6DaTJ09Wbm6uamtrz/ozurq6FIlEYgYAYPCLO4B6enq0YsUKzZ49W1OnTpUktbS0aNSoUUpLS4vZNisrSy0tLWf9ORUVFQoEAtGRk5MTb0sAgAEk7gAqKyvTRx99pNdff/2CGigvL1c4HI6O5ubmC/p5AICBIa4voi5fvlxvv/22tm/frrFjx0aXB4NBdXd3q62tLeYsqLW1tdcv0fn9fvn9/njaAAAMYJ7OgJxzWr58uTZv3qz33ntPeXl5MetnzJihkSNHqqqqKrqsvr5eTU1NKiwsTEzHAIBBwdMZUFlZmTZu3KitW7cqJSUlel0nEAho9OjRCgQCuueee7Ry5Uqlp6crNTVV999/vwoLC7kDDgAQw1MArV27VpJ0/fXXxyxft26d7rrrLknSz3/+cw0bNkyLFi1SV1eXiouL9ctf/jIhzQIABg+fc85ZN3G6SCSiQCBg3caQMnHixLjq3n//fc81n9+y78V1113nuWbXrl2eayRF7+j04o033vBcM3nyZM81X/we3pfxwAMPeK6RpIaGhrjqgNOFw2Glpqb2up5nwQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATMT1G1ExuEyZMiWuulAo5LnmN7/5jeea//73v55rHnvsMc81krRs2TLPNb39tt9z+fOf/+y5Jp7empqaPNcAfYUzIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZ4GCl044039tm+7rjjDs81d955p+cav9/vuUaS2tvbPdesWLHCc82LL77ouebEiROea4D+jDMgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJngYKbR27dq46goKCjzXTJs2zXONz+fzXPPKK694rpGkH/3oR55rPvnkk7j2BQx1nAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAw4XPOOesmTheJRBQIBKzbAABcoHA4rNTU1F7XcwYEADBBAAEATHgKoIqKCs2cOVMpKSnKzMzUggULVF9fH7PN9ddfL5/PFzOWLl2a0KYBAAOfpwCqqalRWVmZ6urq9O677+rEiROaN2+eOjo6Yra79957dejQoehYvXp1QpsGAAx8nn4jamVlZczr9evXKzMzU7t379acOXOiy8eMGaNgMJiYDgEAg9IFXQMKh8OSpPT09Jjlr776qjIyMjR16lSVl5fr2LFjvf6Mrq4uRSKRmAEAGAJcnE6ePOluuukmN3v27Jjlv/rVr1xlZaXbt2+fe+WVV9zll1/uFi5c2OvPWbVqlZPEYDAYjEE2wuHwOXMk7gBaunSpGzdunGtubj7ndlVVVU6Sa2hoOOv6zs5OFw6Ho6O5udl80hgMBoNx4eN8AeTpGtDnli9frrffflvbt2/X2LFjz7ltQUGBJKmhoUETJkw4Y73f75ff74+nDQDAAOYpgJxzuv/++7V582ZVV1crLy/vvDV79+6VJGVnZ8fVIABgcPIUQGVlZdq4caO2bt2qlJQUtbS0SJICgYBGjx6tAwcOaOPGjbrxxht16aWXat++fXrwwQc1Z84cTZ8+PSl/AQDAAOXluo96+Zxv3bp1zjnnmpqa3Jw5c1x6errz+/1u4sSJ7pFHHjnv54CnC4fD5p9bMhgMBuPCx/ne+3kYKQAgKXgYKQCgXyKAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmOh3AeScs24BAJAA53s/73cB1N7ebt0CACABzvd+7nP97JSjp6dHBw8eVEpKinw+X8y6SCSinJwcNTc3KzU11ahDe8zDKczDKczDKczDKf1hHpxzam9vVygU0rBhvZ/njOjDnr6UYcOGaezYsefcJjU1dUgfYJ9jHk5hHk5hHk5hHk6xnodAIHDebfrdR3AAgKGBAAIAmBhQAeT3+7Vq1Sr5/X7rVkwxD6cwD6cwD6cwD6cMpHnodzchAACGhgF1BgQAGDwIIACACQIIAGCCAAIAmBgwAbRmzRpdccUVuuiii1RQUKCdO3dat9TnnnzySfl8vpgxefJk67aSbvv27br55psVCoXk8/m0ZcuWmPXOOT3xxBPKzs7W6NGjVVRUpP3799s0m0Tnm4e77rrrjOOjpKTEptkkqaio0MyZM5WSkqLMzEwtWLBA9fX1Mdt0dnaqrKxMl156qS655BItWrRIra2tRh0nx5eZh+uvv/6M42Hp0qVGHZ/dgAigN954QytXrtSqVav04YcfKj8/X8XFxTp8+LB1a31uypQpOnToUHT85S9/sW4p6To6OpSfn681a9acdf3q1av1/PPP68UXX9SOHTt08cUXq7i4WJ2dnX3caXKdbx4kqaSkJOb4eO211/qww+SrqalRWVmZ6urq9O677+rEiROaN2+eOjo6ots8+OCDeuutt7Rp0ybV1NTo4MGDuuWWWwy7TrwvMw+SdO+998YcD6tXrzbquBduAJg1a5YrKyuLvj558qQLhUKuoqLCsKu+t2rVKpefn2/dhilJbvPmzdHXPT09LhgMup/85CfRZW1tbc7v97vXXnvNoMO+8cV5cM65JUuWuPnz55v0Y+Xw4cNOkqupqXHOnfpvP3LkSLdp06boNv/85z+dJFdbW2vVZtJ9cR6cc+66665zDzzwgF1TX0K/PwPq7u7W7t27VVRUFF02bNgwFRUVqba21rAzG/v371coFNL48eN15513qqmpybolU42NjWppaYk5PgKBgAoKCobk8VFdXa3MzExNmjRJy5Yt05EjR6xbSqpwOCxJSk9PlyTt3r1bJ06ciDkeJk+erNzc3EF9PHxxHj736quvKiMjQ1OnTlV5ebmOHTtm0V6v+t3DSL/os88+08mTJ5WVlRWzPCsrS5988olRVzYKCgq0fv16TZo0SYcOHdJTTz2la6+9Vh999JFSUlKs2zPR0tIiSWc9Pj5fN1SUlJTolltuUV5eng4cOKDvfe97Ki0tVW1trYYPH27dXsL19PRoxYoVmj17tqZOnSrp1PEwatQopaWlxWw7mI+Hs82DJN1xxx0aN26cQqGQ9u3bp+985zuqr6/X7373O8NuY/X7AML/lZaWRv88ffp0FRQUaNy4cXrzzTd1zz33GHaG/uC2226L/nnatGmaPn26JkyYoOrqas2dO9ews+QoKyvTRx99NCSug55Lb/Nw3333Rf88bdo0ZWdna+7cuTpw4IAmTJjQ122eVb//CC4jI0PDhw8/4y6W1tZWBYNBo676h7S0NF111VVqaGiwbsXM58cAx8eZxo8fr4yMjEF5fCxfvlxvv/223n///Zhf3xIMBtXd3a22traY7Qfr8dDbPJxNQUGBJPWr46HfB9CoUaM0Y8YMVVVVRZf19PSoqqpKhYWFhp3ZO3r0qA4cOKDs7GzrVszk5eUpGAzGHB+RSEQ7duwY8sfHp59+qiNHjgyq48M5p+XLl2vz5s167733lJeXF7N+xowZGjlyZMzxUF9fr6ampkF1PJxvHs5m7969ktS/jgfruyC+jNdff935/X63fv16949//MPdd999Li0tzbW0tFi31qceeughV11d7RobG90HH3zgioqKXEZGhjt8+LB1a0nV3t7u9uzZ4/bs2eMkuZ/97Gduz5497t///rdzzrkf//jHLi0tzW3dutXt27fPzZ8/3+Xl5bnjx48bd55Y55qH9vZ29/DDD7va2lrX2Njotm3b5r72ta+5K6+80nV2dlq3njDLli1zgUDAVVdXu0OHDkXHsWPHotssXbrU5ebmuvfee8/t2rXLFRYWusLCQsOuE+9889DQ0OB+8IMfuF27drnGxka3detWN378eDdnzhzjzmMNiAByzrkXXnjB5ebmulGjRrlZs2a5uro665b63OLFi112drYbNWqUu/zyy93ixYtdQ0ODdVtJ9/777ztJZ4wlS5Y4507div3444+7rKws5/f73dy5c119fb1t00lwrnk4duyYmzdvnrvsssvcyJEj3bhx49y999476P6Rdra/vyS3bt266DbHjx933/72t91XvvIVN2bMGLdw4UJ36NAhu6aT4Hzz0NTU5ObMmePS09Od3+93EydOdI888ogLh8O2jX8Bv44BAGCi318DAgAMTgQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEz8D6Cb402/R3CGAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Detailed Breakdown:\n",
        "\n",
        "1. **Transformations**:\n",
        "   - **`transforms.ToTensor()`**: Converts the images from PIL format (values between 0 and 255) to PyTorch tensors (values between 0 and 1).\n",
        "   - **`transforms.Normalize((0.5,), (0.5,))`**: Normalizes the pixel values to the range `[-1, 1]`. The mean and standard deviation are set to `0.5` because the MNIST images are grayscale, and normalizing them helps to speed up and stabilize the training process.\n",
        "\n",
        "2. **Dataset and DataLoader**:\n",
        "   - The `MNIST` dataset is downloaded (if not already present) and loaded into memory. The `train=True` flag loads the training data, while `train=False` loads the test data.\n",
        "   - The `DataLoader` class creates iterators over the dataset in mini-batches of size 64. For training, the data is shuffled to improve generalization, while test data is loaded in order.\n",
        "\n",
        "3. **Data Visualization**:\n",
        "   - A single batch of training data is loaded with `iter(train_loader)`, and `example_data[0][0]` selects the first image from the first batch.\n",
        "   - **`plt.imshow()`** is used to display the image using Matplotlib. Since MNIST images are grayscale, the `cmap='gray'` parameter ensures they are displayed in grayscale.\n",
        "\n",
        "#### Output:\n",
        "The output will display a grayscale image of a handwritten digit from the MNIST dataset.\n",
        "\n",
        "This code is useful for setting up a training pipeline for models using the MNIST dataset, and the DataLoader makes it efficient to work with mini-batches."
      ],
      "metadata": {
        "id": "EpSpCkJJ8O42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **3.3. Custom Datasets**\n",
        "- Often, you’ll need to work with datasets that are not included in PyTorch’s built-in datasets. PyTorch allows you to create custom datasets by subclassing `torch.utils.data.Dataset`.\n",
        "\n",
        "---\n",
        "\n",
        "**3.3.1. Creating a Custom Dataset**\n",
        "- A custom dataset must implement the following methods:\n",
        "  - `__len__()`: Returns the total number of samples.\n",
        "  - `__getitem__(index)`: Retrieves a sample at a specific index.\n",
        "\n",
        "**Demonstration: Custom Dataset for Image Data**"
      ],
      "metadata": {
        "id": "8e6-iEq-2LD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Custom dataset class for loading image data from a folder\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_dir (str): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        # Check if the directory exists\n",
        "        if not os.path.exists(image_dir):\n",
        "            print(f\"Error: The directory '{image_dir}' does not exist.\")\n",
        "            self.image_filenames = []  # Set empty image list to avoid issues\n",
        "        else:\n",
        "            self.image_dir = image_dir\n",
        "            self.image_filenames = os.listdir(image_dir)  # Load image filenames\n",
        "            if len(self.image_filenames) == 0:\n",
        "                print(f\"Error: The directory '{image_dir}' is empty.\")\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)  # Return the total number of images\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return None if the dataset is empty\n",
        "        if len(self.image_filenames) == 0:\n",
        "            return None\n",
        "\n",
        "        # Load image at the given index\n",
        "        img_name = os.path.join(self.image_dir, self.image_filenames[idx])\n",
        "        image = Image.open(img_name)  # Open image using PIL\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # Apply transformations if specified\n",
        "\n",
        "        return image  # Return the processed image\n",
        "\n",
        "# Define transformation: Resize and convert to tensor\n",
        "transform = transforms.Compose([transforms.Resize((128, 128)),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "# Specify the directory for images\n",
        "image_dir = './images'  # Directory where images are stored\n",
        "\n",
        "# Initialize the custom dataset with transformation\n",
        "custom_dataset = CustomImageDataset(image_dir=image_dir, transform=transform)\n",
        "\n",
        "# Check if the dataset has images\n",
        "if len(custom_dataset) == 0:\n",
        "    print(f\"Error: No images found in the directory '{image_dir}'.\")\n",
        "\n",
        "else:\n",
        "    # Load the dataset using DataLoader\n",
        "    custom_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
        "                                                batch_size=32,\n",
        "                                                shuffle=True)\n",
        "\n",
        "    # Iterate through the DataLoader and display one image\n",
        "    for batch in custom_loader:\n",
        "        plt.imshow(batch[0].permute(1, 2, 0))  # Convert the tensor to an image\n",
        "        plt.show()\n",
        "        break  # Display one image and then exit\n"
      ],
      "metadata": {
        "id": "ljWetlMh8O42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec1d1298-6329-4fe1-8d69-927bc97bffa0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The directory './images' does not exist.\n",
            "Error: No images found in the directory './images'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explanation of the Code:\n",
        "\n",
        "This code defines a custom PyTorch dataset class that loads images from a specified folder and uses transformations to preprocess the images. Additionally, it includes error handling for cases where the directory doesn't exist or is empty, allowing for graceful handling of such errors.\n",
        "\n",
        "###### 1. **Custom Dataset Class** (`CustomImageDataset`)\n",
        "\n",
        "- **`__init__` Method**:\n",
        "  - The constructor accepts an image directory (`image_dir`) and an optional transformation (`transform`).\n",
        "  - **Error Handling for Directory**:\n",
        "    - It checks if the specified directory exists using `os.path.exists(image_dir)`.\n",
        "    - If the directory does not exist, it prints a user-friendly error message and sets the list of image filenames (`image_filenames`) to an empty list to avoid further issues.\n",
        "    - If the directory exists but contains no images, it prints a warning that the directory is empty.\n",
        "  - **Image Filenames**: If the directory is valid, it lists all the files in the directory using `os.listdir(image_dir)` and stores them in `self.image_filenames`.\n",
        "\n",
        "- **`__len__` Method**:\n",
        "  - This method returns the number of images in the dataset, which is simply the length of the `image_filenames` list.\n",
        "  - If the directory doesn't exist or is empty, this method returns 0.\n",
        "\n",
        "- **`__getitem__` Method**:\n",
        "  - This method loads an image by its index (`idx`) from `self.image_filenames`. It constructs the full path to the image and loads it using **PIL** (`Image.open()`).\n",
        "  - **Transformations**: If a transformation was passed (e.g., resizing, converting to tensor), it applies the transformation to the image.\n",
        "  - The method returns the processed image.\n",
        "  - **Empty Dataset Handling**: If there are no images in the dataset, it returns `None`, preventing further issues when the dataset is empty.\n",
        "\n",
        "###### 2. **Transformations**:\n",
        "- **Resize and Convert to Tensor**:\n",
        "  - The transformation is applied using `transforms.Compose()`, which first resizes all images to 128x128 pixels and then converts them to PyTorch tensors.\n",
        "  - The `Resize` transformation ensures that all images have the same dimensions, making them compatible for model input.\n",
        "  - The `ToTensor()` transformation converts images to tensors with pixel values normalized between `[0, 1]`, which is the standard format for neural networks in PyTorch.\n",
        "\n",
        "###### 3. **Custom Dataset Initialization**:\n",
        "- The `CustomImageDataset` class is instantiated by specifying the directory (`image_dir`) where the images are stored and the transformation pipeline to preprocess the images. If the directory doesn't exist or is empty, the dataset is gracefully handled by setting an empty image list.\n",
        "\n",
        "###### 4. **Error Handling in Dataset**:\n",
        "- **Directory Not Found**: If the directory specified does not exist, a message is printed to notify the user (`Error: The directory './images' does not exist.`), and no further image processing is attempted.\n",
        "- **Empty Directory**: If the directory is present but contains no images, a message is printed (`Error: The directory './images' is empty.`).\n",
        "\n",
        "###### 5. **Dataset Loader with DataLoader**:\n",
        "- The `DataLoader` class is used to load the dataset in mini-batches, allowing for efficient training and testing.\n",
        "  - **Batch Size**: The images are loaded in batches of 32.\n",
        "  - **Shuffle**: Shuffling ensures that the data is randomly ordered during training, which helps prevent the model from learning the order of the dataset.\n",
        "  - If the dataset is empty (or not available), the program doesn't attempt to load any batches.\n",
        "\n",
        "###### 6. **Iterating and Displaying Images**:\n",
        "- If the dataset contains images, the DataLoader iterates over the dataset and loads one batch of images at a time.\n",
        "- **Image Visualization**:\n",
        "  - The first image from the batch is displayed using Matplotlib (`plt.imshow()`).\n",
        "  - Since PyTorch tensors store image data in `(C, H, W)` format (channels first), the tensor is **permuted** to the format expected by Matplotlib `(H, W, C)` (height, width, channels).\n",
        "\n",
        "###### 7. **Handling an Empty Dataset**:\n",
        "- If the dataset has no images (`len(custom_dataset) == 0`), a message is printed (`Error: No images found in the directory './images'.`), and no further actions are performed, thus avoiding any errors during image loading or processing.\n"
      ],
      "metadata": {
        "id": "uoVblLfT8O8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **3.4. Data Transformations**\n",
        "- Data preprocessing and augmentation are essential steps in preparing datasets for training neural networks.\n",
        "- PyTorch provides a module called `torchvision.transforms` to apply transformations like resizing, cropping, flipping, and normalization.\n",
        "- These transformations can be chained together using `transforms.Compose()`.\n",
        "\n",
        "---\n",
        "\n",
        "**3.4.1. Common Data Transformations**\n",
        "- **Resizing**: Resize an image to a specified size (`transforms.Resize()`).\n",
        "- **Random Cropping**: Randomly crop an image to a specified size (`transforms.RandomCrop()`).\n",
        "- **Flipping**: Randomly flip an image horizontally (`transforms.RandomHorizontalFlip()`).\n",
        "- **Normalization**: Normalize the image tensor by specifying the mean and standard deviation (`transforms.Normalize()`).\n",
        "\n",
        "**Demonstration: Applying Transformations**"
      ],
      "metadata": {
        "id": "_UUUAYUT8O_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a set of transformations to be applied to the images in the dataset.\n",
        "# The transformations are applied sequentially using `transforms.Compose`.\n",
        "transform = transforms.Compose([\n",
        "\n",
        "    # Step 1: Resize the image to a fixed size of 256x256 pixels.\n",
        "    # Resizing is important to ensure that all images are of the same dimensions,\n",
        "    # regardless of their original size, before feeding them into a neural network.\n",
        "    transforms.Resize(256),\n",
        "\n",
        "    # Step 2: Randomly crop a 224x224 section from the resized 256x256 image.\n",
        "    # This adds randomness and slight variations to the images, which acts as data augmentation.\n",
        "    # It helps the model generalize better by simulating different perspectives.\n",
        "    transforms.RandomCrop(224),\n",
        "\n",
        "    # Step 3: Randomly flip the image horizontally with a 50% chance.\n",
        "    # This is another data augmentation technique that increases the diversity of training data.\n",
        "    # It helps the model handle variations in image orientation.\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "\n",
        "    # Step 4: Convert the image from a PIL image to a PyTorch tensor.\n",
        "    # This is a necessary step before feeding the image into a neural network in PyTorch.\n",
        "    # The image's pixel values are scaled to the range [0, 1].\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Step 5: Normalize the tensor.\n",
        "    # This adjusts the pixel values to have a mean of 0 and a standard deviation of 1.\n",
        "    # The normalization formula is: `(pixel_value - mean) / std`.\n",
        "    # Here, the mean and std values for each channel (RGB) are [0.5, 0.5, 0.5],\n",
        "    # so the pixel values are normalized to the range [-1, 1].\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Apply the defined transformations to the custom dataset.\n",
        "# The dataset will now automatically apply the specified transformations (resize, crop, flip, etc.)\n",
        "# to each image when loaded.\n",
        "transformed_dataset = CustomImageDataset(image_dir='./images', transform=transform)\n",
        "\n",
        "\n",
        "# Specify the directory for images\n",
        "image_dir = './images'  # Directory where images are stored\n",
        "\n",
        "# Initialize the custom dataset with transformation\n",
        "custom_dataset = CustomImageDataset(image_dir=image_dir, transform=transform)\n",
        "\n",
        "# Check if the dataset has images\n",
        "if len(custom_dataset) == 0:\n",
        "    print(f\"Error: No images found in the directory '{image_dir}'.\")\n",
        "\n",
        "else:\n",
        "    # Load the transformed dataset using PyTorch's DataLoader.\n",
        "    # The DataLoader loads the dataset in batches of size 16 and shuffles the data after each epoch.\n",
        "    # - `batch_size=16`: Load the data in batches of 16 images.\n",
        "    # - `shuffle=True`: Shuffle the dataset to randomize the order of the images in each epoch.\n",
        "    data_loader = torch.utils.data.DataLoader(custom_dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "    # Iterate through the DataLoader and display one sample image.\n",
        "    # The DataLoader provides batches of images, and in this case, we'll display the first image from the first batch.\n",
        "    for batch in data_loader:\n",
        "        # Step 1: Convert the image tensor to a format suitable for displaying using Matplotlib.\n",
        "        # The PyTorch tensor format is (C, H, W), but Matplotlib expects (H, W, C), so we use permute().\n",
        "        plt.imshow(batch[0].permute(1, 2, 0))  # Convert (C, H, W) to (H, W, C).\n",
        "\n",
        "        # Step 2: Display the image using Matplotlib.\n",
        "        plt.show()\n",
        "\n",
        "        # Break after displaying one image, so the loop doesn't keep running.\n",
        "        break\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fyv-5sn-8O_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b844842-892a-4f9d-e3d6-e169b5669254"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: The directory './images' does not exist.\n",
            "Error: The directory './images' does not exist.\n",
            "Error: No images found in the directory './images'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is what the above code does:\n",
        "\n",
        "1. **Transformation Pipeline** (`transforms.Compose`):\n",
        "   - **Resize** (`transforms.Resize(256)`):\n",
        "     - This transformation resizes all images to a consistent size of 256x256 pixels, ensuring that images are of the same dimensions before being fed into the model. This is crucial because neural networks generally expect inputs to have the same size.\n",
        "   \n",
        "   - **RandomCrop** (`transforms.RandomCrop(224)`):\n",
        "     - After resizing the image to 256x256, this transformation randomly crops a 224x224 section from the image. Cropping adds randomness, which acts as **data augmentation**. This technique helps the model generalize better by showing it different parts of the image during training.\n",
        "\n",
        "   - **RandomHorizontalFlip** (`transforms.RandomHorizontalFlip()`):\n",
        "     - This randomly flips the image horizontally with a 50% probability. It helps create additional variations of the images, improving the model’s robustness by simulating mirrored views. This is especially useful when the horizontal orientation of the object in the image doesn't affect the classification.\n",
        "\n",
        "   - **ToTensor** (`transforms.ToTensor()`):\n",
        "     - Converts the PIL image (from libraries like OpenCV or Pillow) to a **PyTorch tensor**. The pixel values, which are originally in the range `[0, 255]`, are scaled to `[0, 1]` (floating point values). This step is necessary because neural networks in PyTorch expect input data to be in tensor format.\n",
        "\n",
        "   - **Normalize** (`transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])`):\n",
        "     - Normalizes the pixel values of the tensor. The normalization formula is `(pixel_value - mean) / std` applied to each color channel (Red, Green, Blue). With mean `[0.5, 0.5, 0.5]` and std `[0.5, 0.5, 0.5]`, the pixel values are normalized to the range `[-1, 1]`. This helps stabilize the learning process by standardizing input data.\n",
        "\n",
        "2. **Custom Dataset (`CustomImageDataset`)**:\n",
        "   - This class applies the defined transformation pipeline to each image in the specified directory (`'./images'`). When the dataset is accessed through the DataLoader, the images are resized, randomly cropped, flipped, and normalized automatically.\n",
        "\n",
        "3. **DataLoader**:\n",
        "   - **Batch Loading** (`batch_size=16`):\n",
        "     - The DataLoader groups the dataset into mini-batches of size 16. This means that during training or testing, 16 images are processed at a time, which speeds up computation and takes advantage of parallelism.\n",
        "   \n",
        "   - **Shuffle** (`shuffle=True`):\n",
        "     - Shuffling randomizes the order of the images at the start of each epoch, which helps the model generalize better by ensuring it doesn’t memorize the sequence of the images during training.\n",
        "\n",
        "4. **Image Visualization**:\n",
        "   - The code iterates over the DataLoader and extracts the first image from the first mini-batch.\n",
        "   \n",
        "   - **Tensor Dimension Permutation** (`batch[0].permute(1, 2, 0)`):\n",
        "     - PyTorch tensors representing images are typically in the format `(C, H, W)` (channels first). However, Matplotlib expects images in the format `(H, W, C)` (height, width, channels). The `permute(1, 2, 0)` operation reorders the dimensions to make the tensor compatible with Matplotlib for display.\n",
        "\n",
        "   - **Display Image**:\n",
        "     - The first image in the batch is displayed using `plt.imshow()`, which renders the image in a window. The loop is broken after displaying one image to prevent further iterations.\n"
      ],
      "metadata": {
        "id": "9XCqUFJv8PBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **3.5. DataLoader for Efficient Data Loading**\n",
        "- **Batching**: DataLoader breaks the dataset into mini-batches, allowing for efficient training, especially on large datasets.\n",
        "- **Shuffling**: Randomly shuffling the dataset helps prevent the model from learning the order of the data, improving generalization.\n",
        "- **Parallel Data Loading**: By using the `num_workers` parameter, DataLoader can load data in parallel using multiple CPU cores, speeding up the data loading process.\n",
        "\n",
        "---\n",
        "\n",
        "**3.5.1. DataLoader with Parallel Processing**\n",
        "- Loading large datasets can be time-consuming, especially when working with high-resolution images or large text datasets.\n",
        "- `DataLoader` supports parallel data loading using multiple CPU workers, which can significantly speed up data preparation.\n",
        "\n",
        "**Demonstration: DataLoader with Multiple Workers**"
      ],
      "metadata": {
        "id": "PnxWm2n_8PF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST dataset with DataLoader, using 4 workers for parallel data loading.\n",
        "# The DataLoader efficiently loads the dataset in mini-batches, shuffles the data,\n",
        "# and can load data in parallel using multiple worker threads.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,   # The dataset to load, assumed to be the MNIST training dataset.\n",
        "    batch_size=64,           # Load 64 samples (images) in each batch.\n",
        "    shuffle=True,            # Shuffle the dataset at the beginning of each epoch, ensuring randomness.\n",
        "    num_workers=4            # Use 4 parallel workers to load data concurrently, speeding up loading time.\n",
        ")\n",
        "\n",
        "# Iterate through the DataLoader, which loads the dataset in mini-batches.\n",
        "# For each batch, 'data' contains the batch of images, and 'target' contains the corresponding labels.\n",
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "    # Print information about the current batch.\n",
        "    # 'data.shape' gives the shape of the input images in the batch, which should be [64, 1, 28, 28] for MNIST.\n",
        "    # 'target.shape' gives the shape of the labels, which should be [64] (one label per image in the batch).\n",
        "    print(f'Batch {batch_idx + 1}: Data Shape: {data.shape}, Target Shape: {target.shape}')\n",
        "\n",
        "    # Stop the iteration after printing 3 batches (batch_idx starts at 0).\n",
        "    if batch_idx == 2:  # This ensures that only the first 3 batches are processed and printed.\n",
        "        break  # Exit the loop after processing and displaying 3 batches.\n"
      ],
      "metadata": {
        "id": "NJMStppx8PF3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3323fa1-553f-4237-b84d-8d23b4b7af42"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Data Shape: torch.Size([64, 1, 28, 28]), Target Shape: torch.Size([64])\n",
            "Batch 2: Data Shape: torch.Size([64, 1, 28, 28]), Target Shape: torch.Size([64])\n",
            "Batch 3: Data Shape: torch.Size([64, 1, 28, 28]), Target Shape: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Explanation:\n",
        "\n",
        "1. **DataLoader Setup**:\n",
        "   - **`dataset=train_dataset`**: Refers to the MNIST training dataset that was defined earlier.\n",
        "   - **`batch_size=64`**: Specifies that each mini-batch will contain 64 samples (i.e., 64 images and their corresponding labels).\n",
        "     - For MNIST, the images are grayscale and have dimensions `[1, 28, 28]`, so the `data` tensor for each batch will have the shape `[64, 1, 28, 28]`.\n",
        "   - **`shuffle=True`**: Shuffles the dataset at the start of each epoch to randomize the order of the images. This helps improve the model's generalization by preventing it from learning the order of the data.\n",
        "   - **`num_workers=4`**: Utilizes 4 worker threads for parallel data loading, which can speed up the process of reading and processing data, especially for larger datasets.\n",
        "\n",
        "2. **DataLoader Iteration**:\n",
        "   - **`enumerate(train_loader)`**: Iterates over the `train_loader`, returning the batch index (`batch_idx`) and the corresponding batch of data (`data`, `target`). Each iteration retrieves one mini-batch of data.\n",
        "   - **`data`**: This tensor contains the batch of images from the dataset. For MNIST, the shape is expected to be `[64, 1, 28, 28]` where:\n",
        "     - `64` is the batch size (number of images).\n",
        "     - `1` represents the number of channels (grayscale).\n",
        "     - `28x28` are the height and width of the images.\n",
        "   - **`target`**: This tensor contains the labels corresponding to the images. Its shape is `[64]`, meaning one label for each image in the batch.\n",
        "\n",
        "3. **Printing Batch Information**:\n",
        "   - **`print(f'Batch {batch_idx + 1}: Data Shape: {data.shape}, Target Shape: {target.shape}')`**: This prints the current batch number (`batch_idx + 1` since `batch_idx` is zero-based) and the shapes of the `data` and `target` tensors.\n",
        "\n",
        "4. **Exiting the Loop**:\n",
        "   - The loop stops after 3 batches are printed (`batch_idx == 2`). The `break` statement ensures that after processing and printing the shapes of 3 batches, the iteration exits, preventing further processing.\n",
        "\n",
        "##### Output Example:\n",
        "If you were to run the code, you would see output similar to the following for the first three batches:\n",
        "\n",
        "```bash\n",
        "Batch 1: Data Shape: torch.Size([64, 1, 28, 28]), Target Shape: torch.Size([64])\n",
        "Batch 2: Data Shape: torch.Size([64, 1, 28, 28]), Target Shape: torch.Size([64])\n",
        "Batch 3: Data Shape: torch.Size([64, 1, 28, 28]), Target Shape: torch.Size([64])\n",
        "```\n",
        "\n",
        "- **Data Shape**: `[64, 1, 28, 28]` represents a batch of 64 grayscale images of size 28x28.\n",
        "- **Target Shape**: `[64]` means that there are 64 labels, one for each image in the batch.\n",
        "\n",
        "##### Summary:\n",
        "- The `DataLoader` loads the MNIST dataset in mini-batches of 64 images.\n",
        "- It uses 4 workers for parallel data loading to improve efficiency.\n",
        "- The loop iterates over the batches, printing the shape of the data and target tensors for the first 3 batches."
      ],
      "metadata": {
        "id": "ichC1eAi8PJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **3.6. Observations on State-of-the-Art Research**\n",
        "- **Large-scale Data Handling**:\n",
        "  - In state-of-the-art deep learning models, especially those trained on large datasets like ImageNet, the efficient handling of data loading and preprocessing has become crucial for reducing training times.\n",
        "  - Techniques like **distributed data loading** (splitting data across multiple GPUs/nodes) are now common in large-scale AI research.\n",
        "- **Data Augmentation**:\n",
        "  - Data augmentation has been proven to significantly improve model generalization. For example, in computer vision, **Mixup** and **CutMix** have been introduced as advanced augmentation techniques that improve robustness.\n",
        "- **Synthetic Data Generation**:\n",
        "  - Due to the limitations of data availability in some domains, researchers are increasingly using **synthetic data** generated through techniques like **GANs (Generative Adversarial Networks)** to train models.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "c6SBy-6b8PNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Continuity to the Next Section\n",
        "- In the next section, we will dive into **training neural networks** using the knowledge of datasets and DataLoader that we've covered.\n",
        "- We will explore training loops, calculating loss, backpropagation, and monitoring model performance during training.\n",
        "\n",
        "This section covered loading and preprocessing datasets using PyTorch's `Dataset` and `DataLoader`, providing the tools necessary to handle large datasets and implement data augmentation. With efficient data handling, we can now proceed to the actual training process of neural networks in the next section."
      ],
      "metadata": {
        "id": "oSJ3t8fK8PQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations"
      ],
      "metadata": {
        "id": "aBe8F11xVhxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observation 1: Efficient Handling of Large Datasets with Memory Constraints**\n",
        "- When working with very large datasets, loading the entire dataset into memory may not be feasible.\n",
        "- Instead, **lazy loading** or loading data in small chunks can be beneficial.\n",
        "- In PyTorch, `DataLoader` handles this by loading data in mini-batches, rather than loading the entire dataset at once.\n",
        "\n",
        "##### Demonstration: Lazy Loading with DataLoader\n"
      ],
      "metadata": {
        "id": "Cgs7VxuxV8mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Import the 'torch' library to access its functionalities, such as creating DataLoaders.\n",
        "\n",
        "# Use DataLoader to load mini-batches of data from the dataset, rather than loading the entire dataset at once.\n",
        "# This helps in managing memory efficiently, especially when working with large datasets.\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,  # The dataset to load (assumed to be pre-defined, such as the MNIST training dataset).\n",
        "    batch_size=64,          # Specifies that 64 samples (images) will be loaded in each batch.\n",
        "    shuffle=True,           # Shuffle the data before each epoch to ensure randomness, improving the model's generalization.\n",
        "    num_workers=2           # Use 2 worker threads to load data in parallel. This helps in speeding up the data-loading process.\n",
        ")\n",
        "\n",
        "# Iterate through the DataLoader, which will load the data in mini-batches rather than all at once.\n",
        "# This allows for efficient memory usage and faster iteration, as only the required batch of data is loaded at a time.\n",
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "    # For each batch, print the batch number and the shape of the data and target tensors.\n",
        "    # 'data.shape' will print the shape of the batch of images, which will be [64, 1, 28, 28] for MNIST.\n",
        "    # 'target.shape' will print the shape of the labels, which will be [64] (one label for each image in the batch).\n",
        "    print(f'Batch {batch_idx + 1}: Data Shape: {data.shape}, Target Shape: {target.shape}')\n",
        "\n",
        "    # Stop the loop after processing 3 batches (batch_idx starts at 0, so this breaks after the third batch).\n",
        "    if batch_idx == 2:\n",
        "        break  # Exit the loop after printing 3 batches.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-evkmpRoZ5D",
        "outputId": "490bb4a5-7be0-435e-a958-b7964ef68253"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Data Shape: torch.Size([64, 1, 28, 28]), Target Shape: torch.Size([64])\n",
            "Batch 2: Data Shape: torch.Size([64, 1, 28, 28]), Target Shape: torch.Size([64])\n",
            "Batch 3: Data Shape: torch.Size([64, 1, 28, 28]), Target Shape: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observation 2: Dataset Length Does Not Have to Match DataLoader Batch Size**\n",
        "- If the dataset size is not divisible by the batch size, PyTorch handles this gracefully, ensuring that the final batch contains only the remaining samples without errors.\n",
        "\n",
        "##### Demonstration: Handling the Last Partial Batch\n"
      ],
      "metadata": {
        "id": "Tu21sEb3oamH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DataLoader to load data in mini-batches, even if the total dataset size is not divisible by the batch size.\n",
        "# PyTorch's DataLoader will automatically handle the last batch, ensuring it's properly loaded even if it's smaller.\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,  # Load the specified dataset (e.g., MNIST or another training dataset).\n",
        "    batch_size=128,         # Set a large batch size (128 samples), which might not evenly divide the dataset size.\n",
        "    shuffle=True            # Shuffle the data at each epoch to ensure random order during training.\n",
        ")\n",
        "\n",
        "# Iterate through the DataLoader to process batches of data.\n",
        "# We'll check the size of the last batch to demonstrate how PyTorch handles it if it is smaller than the specified batch size.\n",
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "    # If we're at the last batch (when batch_idx equals len(train_loader) - 1), print the batch size.\n",
        "    if batch_idx == len(train_loader) - 1:\n",
        "        # 'data.shape' gives the shape of the input images in the batch.\n",
        "        # The last batch may have fewer samples than 128, and PyTorch handles this without issues.\n",
        "        print(f'Last Batch Size: {data.shape}')  # Print the size of the last batch (might be smaller than 128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No1oE5ZloamH",
        "outputId": "2a55dbd8-2814-409d-b797-b5a1b0232687"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last Batch Size: torch.Size([96, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observation 4: Augmenting Data Dynamically During Training**\n",
        "- Data augmentation can be applied dynamically during training, which increases dataset variability without increasing memory usage. This allows for augmenting the data differently with each epoch.\n",
        "\n",
        "##### Demonstration: Dynamic Data Augmentation with `transforms`\n"
      ],
      "metadata": {
        "id": "TJ4vQPMeoauk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dynamic transformations for data augmentation during training.\n",
        "# These transformations will be applied to each sample dynamically, meaning different augmentations can be applied to the same image across different epochs.\n",
        "transform = transforms.Compose([\n",
        "\n",
        "    # Randomly crop the image to 224x224 pixels, resizing it as necessary.\n",
        "    transforms.RandomResizedCrop(224),\n",
        "\n",
        "    # Randomly flip the image horizontally with a 50% chance.\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "\n",
        "    # Randomly change the brightness and contrast of the image.\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "\n",
        "    # Convert the image to a PyTorch tensor.\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "    # Normalize the tensor with a mean of 0.5 and standard deviation of 0.5.\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Apply the dynamic transformations to the MNIST training dataset.\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "# Load the training dataset in mini-batches of 64 images each.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Add print statements to better understand the data loading process\n",
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    # Print the current batch index to keep track of the progress\n",
        "    print(f'Processing batch {batch_idx + 1}')\n",
        "\n",
        "    # Print the shape of the data tensor to verify the batch size and dimensions (expected shape: [64, 1, 224, 224])\n",
        "    print(f'Batch {batch_idx + 1} - Data shape: {data.shape}')\n",
        "\n",
        "    # Print the shape of the target tensor to verify the number of labels in the batch\n",
        "    print(f'Batch {batch_idx + 1} - Target shape: {target.shape}')\n",
        "\n",
        "    # Break after processing a few batches to avoid excessive output\n",
        "    if batch_idx == 2:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBts9WG2oaul",
        "outputId": "4fb224d4-3f3c-4eb5-dadc-6e09d0245750"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing batch 1\n",
            "Batch 1 - Data shape: torch.Size([64, 1, 224, 224])\n",
            "Batch 1 - Target shape: torch.Size([64])\n",
            "Processing batch 2\n",
            "Batch 2 - Data shape: torch.Size([64, 1, 224, 224])\n",
            "Batch 2 - Target shape: torch.Size([64])\n",
            "Processing batch 3\n",
            "Batch 3 - Data shape: torch.Size([64, 1, 224, 224])\n",
            "Batch 3 - Target shape: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observation 6: Custom Dataset Classes Can Handle Multiple Input Modalities**\n",
        "- Custom `Dataset` classes can handle multiple input modalities, such as combining image data with tabular or text data, which can be useful for multimodal models.\n",
        "\n",
        "##### Demonstration: Custom Dataset Handling Multiple Inputs (Images and Metadata)\n"
      ],
      "metadata": {
        "id": "Ly8X2SLkoa2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from PIL import Image\n",
        "import pandas as pd # Import the pandas library and assign it the alias 'pd'\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "\n",
        "class MultiModalDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, metadata_file, transform=None):\n",
        "        # Initialize the dataset by specifying the directory where images are stored and loading metadata.\n",
        "        # 'image_dir' is the folder containing images, and 'metadata_file' is a CSV file with additional features.\n",
        "\n",
        "        self.image_dir = image_dir  # Path to the directory containing images.\n",
        "\n",
        "        # Read the metadata CSV file using pandas. The file is assumed to have at least one column for image filenames\n",
        "        # and additional columns containing metadata (e.g., numerical or categorical features).\n",
        "        # Check if the metadata file exists\n",
        "        if os.path.exists(metadata_file):\n",
        "            self.metadata = pd.read_csv(metadata_file)\n",
        "        else:\n",
        "            print(f\"Error: Metadata file not found at path: {metadata_file}\")\n",
        "            # You can choose to handle this error differently,\n",
        "            # like raising an exception or initializing an empty metadata.\n",
        "            # For now, we'll initialize an empty DataFrame.\n",
        "            self.metadata = pd.DataFrame()\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of data samples in the dataset. The number of samples is equal to the number of rows in the metadata.\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve a data sample (image and corresponding metadata) by its index.\n",
        "\n",
        "        # Get the image filename from the metadata and construct the full image path.\n",
        "        image_path = os.path.join(self.image_dir, self.metadata.iloc[idx]['image_filename'])\n",
        "\n",
        "        # Open the image using PIL.\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Apply any specified transformations to the image, if a transform function is provided.\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Extract the additional metadata (features) from the corresponding row in the CSV.\n",
        "        # Convert the metadata (excluding the image filename) to a PyTorch tensor of type float32.\n",
        "        metadata = torch.tensor(self.metadata.iloc[idx, 1:].values, dtype=torch.float32)\n",
        "\n",
        "        # Return the image and the corresponding metadata as a tuple.\n",
        "        return image, metadata\n",
        "\n",
        "# Example usage of the MultiModalDataset class\n",
        "# This dataset loads both images and associated metadata features from a CSV file.\n",
        "dataset = MultiModalDataset(\n",
        "    image_dir='./images',          # Directory containing the images.\n",
        "    metadata_file='./metadata.csv',  # CSV file containing image filenames and additional metadata features.\n",
        "    transform=transform            # Optional transformations (e.g., resizing or normalizing images).\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKqmFoyVoa2h",
        "outputId": "b2edf613-34ed-4fe9-fd58-f3897677e683"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Metadata file not found at path: ./metadata.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **Observation 8: Using `Subset` to Split Data for Cross-Validation**\n",
        "- PyTorch allows creating subsets of datasets using `torch.utils.data.Subset`, making it easy to implement cross-validation splits.\n",
        "\n",
        "##### Demonstration: Using `Subset` for Cross-Validation\n"
      ],
      "metadata": {
        "id": "MBD9xhrOobbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create indices to split the dataset into training and validation sets.\n",
        "train_size = int(0.8 * len(train_dataset))  # Compute the size for the training set (80% of the total dataset).\n",
        "val_size = len(train_dataset) - train_size  # Compute the size for the validation set (remaining 20%).\n",
        "\n",
        "print(f\"Training set size: {train_size}\")  # Print the size of the training set.\n",
        "print(f\"Validation set size: {val_size}\")  # Print the size of the validation set.\n",
        "\n",
        "# Randomly split the dataset into training and validation subsets.\n",
        "train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for the training subset.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_subset,  # The subset of data to use for training.\n",
        "    batch_size=64,         # Load data in batches of 64 samples.\n",
        "    shuffle=True           # Shuffle the data to randomize the order of the samples in each epoch.\n",
        ")\n",
        "print(f\"Training DataLoader created with batch size: {train_loader.batch_size}\")  # Confirm DataLoader creation.\n",
        "\n",
        "# Create DataLoader for the validation subset.\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset=val_subset,    # The subset of data to use for validation.\n",
        "    batch_size=64,         # Load validation data in batches of 64 samples.\n",
        "    shuffle=False          # Do not shuffle validation data.\n",
        ")\n",
        "print(f\"Validation DataLoader created with batch size: {val_loader.batch_size}\")  # Confirm DataLoader creation.\n",
        "\n",
        "# Optionally, we can iterate through the first batch of each DataLoader to verify data loading.\n",
        "# This prints information about the first batch of the training and validation sets.\n",
        "\n",
        "# Check the first batch of the training data\n",
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    print(f\"\\nTraining Batch {batch_idx + 1} - Data Shape: {data.shape}, Target Shape: {target.shape}\")\n",
        "    break  # Only print the first batch for demonstration.\n",
        "\n",
        "# Check the first batch of the validation data\n",
        "for batch_idx, (data, target) in enumerate(val_loader):\n",
        "    print(f\"\\nValidation Batch {batch_idx + 1} - Data Shape: {data.shape}, Target Shape: {target.shape}\")\n",
        "    break  # Only print the first batch for demonstration.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHk4BOgbobbk",
        "outputId": "6db2a7e4-f133-4af1-d828-a2f335d8ebca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 48000\n",
            "Validation set size: 12000\n",
            "Training DataLoader created with batch size: 64\n",
            "Validation DataLoader created with batch size: 64\n",
            "\n",
            "Training Batch 1 - Data Shape: torch.Size([64, 1, 224, 224]), Target Shape: torch.Size([64])\n",
            "\n",
            "Validation Batch 1 - Data Shape: torch.Size([64, 1, 224, 224]), Target Shape: torch.Size([64])\n"
          ]
        }
      ]
    }
  ]
}