{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAziM/HVyerxMWAEQFEyqs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/PyTorch-Learning-Repository/blob/main/02_Building_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "42E2pL3y8NyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.1. Introduction to Neural Networks**\n",
        "- Neural networks consist of **layers** of connected neurons, where each neuron performs a weighted sum of its inputs and applies an activation function.\n",
        "- A simple neural network includes:\n",
        "  - **Input layer**: Takes in the input data.\n",
        "  - **Hidden layer(s)**: Transforms the input using weights, biases, and activation functions.\n",
        "  - **Output layer**: Produces the final prediction or classification.\n",
        "- Each neuron has a set of **weights** and **biases** which are learned during training using an **optimizer** (e.g., gradient descent) and a **loss function**.\n"
      ],
      "metadata": {
        "id": "vdtH8THLB1cu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 1: **Do neurons exist in the input and output layers?**\n",
        "\n",
        "##### Answer:\n",
        "Yes, neurons exist in both the **input** and **output layers**, but they serve different purposes:\n",
        "\n",
        "- **Input Layer**:\n",
        "  - The neurons in the input layer represent the features of the input data.\n",
        "  - These neurons do not perform any computations (no activation functions, no weights, or biases).\n",
        "  - They simply pass the input data to the next layer (typically a hidden layer).\n",
        "  \n",
        "  For example, if you have 10 features in your input data, you have 10 neurons in the input layer.\n",
        "\n",
        "- **Output Layer**:\n",
        "  - The output layer consists of neurons that generate the final prediction or classification.\n",
        "  - The number of neurons depends on the type of task:\n",
        "    - **Regression**: One neuron representing the predicted value.\n",
        "    - **Binary classification**: One neuron representing the probability or label of a class.\n",
        "    - **Multi-class classification**: One neuron per class, representing the probability of each class.\n",
        "\n",
        "In summary:\n",
        "- The **input layer** holds raw input features.\n",
        "- The **output layer** produces final predictions or classifications.\n"
      ],
      "metadata": {
        "id": "2yOCNQP2GLEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Question 2: **Do weights, biases, and activation functions exist for the input and output layers?**\n",
        "\n",
        "##### Answer:\n",
        "- **Input Layer**:\n",
        "  - **Weights and biases**: The input layer does not have weights or biases. It simply passes the input data to the next layer.\n",
        "  - **Activation function**: No activation function is applied in the input layer.\n",
        "  \n",
        "- **Output Layer**:\n",
        "  - **Weights and biases**: The output layer does have weights and biases, which are learned during training.\n",
        "  - **Activation function**: The activation function used in the output layer depends on the task:\n",
        "    - **Regression**: Typically no activation function, or sometimes a linear activation.\n",
        "    - **Binary classification**: A **sigmoid** activation function is often used.\n",
        "    - **Multi-class classification**: A **softmax** activation function is commonly used to produce class probabilities.\n",
        "\n",
        "In summary:\n",
        "- **Input layer**: No weights, biases, or activation functions.\n",
        "- **Output layer**: Has weights, biases, and an activation function suited to the task.\n",
        "\n"
      ],
      "metadata": {
        "id": "K7QB2R7HGLB7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "FapEAI7zGJjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.2. PyTorch `torch.nn.Module`: Building Blocks for Neural Networks**\n",
        "- `torch.nn.Module` is the base class for all neural network modules in PyTorch.\n",
        "- Neural networks in PyTorch are built by subclassing `torch.nn.Module` and defining the network architecture in the `__init__` and `forward()` methods.\n",
        "\n",
        "---\n",
        "\n",
        "**2.2.1. Building a Simple Neural Network from Scratch**\n",
        "\n",
        "  **Demonstration: Basic Neural Network Implementation**"
      ],
      "metadata": {
        "id": "6_6QOYPQB3Rv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple neural network class with one hidden layer.\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()  # Call the base class constructor\n",
        "\n",
        "        # Step 1: Define the layers.\n",
        "        # Hidden layer: Input features (3), Output features (5).\n",
        "        self.hidden = nn.Linear(3, 5)  # Fully connected layer: input size=3, output size=5\n",
        "\n",
        "        # Output layer: Input features (5 from the hidden layer), Output features (1).\n",
        "        self.output = nn.Linear(5, 1)  # Fully connected layer: input size=5, output size=1\n",
        "\n",
        "    # Define the forward pass, which describes how the data flows through the network.\n",
        "    def forward(self, x):\n",
        "        # Apply the hidden layer, followed by ReLU activation\n",
        "        x = self.hidden(x)\n",
        "        print(f\"After hidden layer (raw output): {x}\")  # Print the raw output from the hidden layer\n",
        "\n",
        "        x = torch.relu(x)  # Apply ReLU non-linearity\n",
        "        print(f\"After ReLU activation: {x}\")  # Print the output after ReLU activation\n",
        "\n",
        "        # Pass to the output layer (no activation for regression)\n",
        "        x = self.output(x)\n",
        "        print(f\"Final output (before returning): {x}\")  # Print the final output before returning\n",
        "        return x  # Return the final output value\n",
        "\n",
        "# Step 4: Initialize the model by creating an instance of the SimpleNN class.\n",
        "model = SimpleNN()\n",
        "print(\"Model initialized. Here's the model structure:\")\n",
        "print(model)  # Print the model architecture for better understanding\n",
        "\n",
        "# Step 5: Define a sample input tensor (2 samples, 3 features each).\n",
        "input_data = torch.tensor([[0.5, -1.2, 3.0], [0.8, 0.3, -0.7]])\n",
        "print(f\"Input data:\\n{input_data}\")  # Print the input data for clarity\n",
        "\n",
        "# Step 6: Perform a forward pass through the model and store the result in 'output'.\n",
        "output = model(input_data)\n",
        "\n",
        "# Step 7: Print the output, which contains the model's predictions for the input batch.\n",
        "print(f\"Predicted output:\\n{output}\")  # Output: predicted values for the input batch\n"
      ],
      "metadata": {
        "id": "SCBy22qK8N_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f361718a-4218-401e-cf6c-46eb67ce3675"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialized. Here's the model structure:\n",
            "SimpleNN(\n",
            "  (hidden): Linear(in_features=3, out_features=5, bias=True)\n",
            "  (output): Linear(in_features=5, out_features=1, bias=True)\n",
            ")\n",
            "Input data:\n",
            "tensor([[ 0.5000, -1.2000,  3.0000],\n",
            "        [ 0.8000,  0.3000, -0.7000]])\n",
            "After hidden layer (raw output): tensor([[ 0.7752, -1.6480, -0.8435, -0.1251, -0.3348],\n",
            "        [-0.1911,  1.1290,  0.0403, -0.6626,  0.1418]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.7752, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.1290, 0.0403, 0.0000, 0.1418]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[-0.0005],\n",
            "        [-0.2281]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[-0.0005],\n",
            "        [-0.2281]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - **Model Definition**: The neural network is defined as a class inheriting from `nn.Module`.\n",
        "  - **Layers**:\n",
        "    - `nn.Linear(3, 5)` creates a fully connected layer with 3 input features and 5 output features (hidden layer).\n",
        "    - `nn.Linear(5, 1)` creates the output layer with 1 output feature.\n",
        "  - **Activation Function**: ReLU (Rectified Linear Unit) is used for non-linearity after the hidden layer.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1No9b3ac8O0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.3. Forward and Backward Propagation**\n",
        "- **Forward Propagation**: The process of passing the input data through the network layers to obtain the output prediction.\n",
        "- **Backward Propagation**: After calculating the loss, we compute the gradients of the weights and biases using backpropagation, allowing the optimizer to update them.\n",
        "\n",
        "**2.3.1. Performing Forward and Backward Propagation**\n",
        "- PyTorch automatically tracks all operations on tensors with `requires_grad=True`, and gradients are computed with `.backward()`.\n",
        "\n",
        "  **Demonstration: Forward and Backward Propagation**"
      ],
      "metadata": {
        "id": "-wAmyNiTB836"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Initialize the network, loss function, and optimizer\n",
        "model = SimpleNN()\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic Gradient Descent\n",
        "\n",
        "# Step 3: Define input data and target (for supervised learning)\n",
        "input_data = torch.tensor([[0.5, -1.2, 3.0], [0.8, 0.3, -0.7]], requires_grad=True)  # Input features\n",
        "target = torch.tensor([[1.0], [2.0]])  # Actual target values\n",
        "\n",
        "# Step 4: Perform Forward Propagation\n",
        "output = model(input_data)\n",
        "print(\"Predicted output before training:\")\n",
        "print(output)  # Predicted output before training (random weights, so the output won't match the target)\n",
        "\n",
        "# Step 5: Calculate the loss (difference between predicted and target)\n",
        "loss = criterion(output, target)\n",
        "print(\"\\nLoss before backpropagation:\")\n",
        "print(loss)\n",
        "\n",
        "# Step 6: Perform Backward Propagation\n",
        "optimizer.zero_grad()  # Zero out previous gradients (important step)\n",
        "loss.backward()  # Compute gradients using backpropagation\n",
        "\n",
        "# Step 7: Print gradients before updating\n",
        "print(\"\\nGradients before optimization step:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"{name}.grad: {param.grad}\")\n",
        "\n",
        "# Step 8: Perform optimization step to update model parameters\n",
        "optimizer.step()  # Update model weights using the computed gradients\n",
        "\n",
        "# Step 9: Perform another forward pass after the parameters have been updated\n",
        "output_after = model(input_data)\n",
        "print(\"\\nPredicted output after one optimization step:\")\n",
        "print(output_after)\n",
        "\n",
        "# Step 10: Recalculate the loss after the update\n",
        "loss_after = criterion(output_after, target)\n",
        "print(\"\\nLoss after one optimization step:\")\n",
        "print(loss_after)"
      ],
      "metadata": {
        "id": "rlqltPd18O0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c6d0f40-fcfe-4d7b-99ec-fb72e567ce8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After hidden layer (raw output): tensor([[ 0.2619, -0.4934, -0.6943, -1.1839, -0.5413],\n",
            "        [-0.6369,  0.6041, -0.6934,  0.2186,  0.5022]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.2619, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.6041, 0.0000, 0.2186, 0.5022]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[-0.3560],\n",
            "        [-0.5148]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output before training:\n",
            "tensor([[-0.3560],\n",
            "        [-0.5148]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss before backpropagation:\n",
            "tensor(4.0815, grad_fn=<MseLossBackward0>)\n",
            "\n",
            "Gradients before optimization step:\n",
            "hidden.weight.grad: tensor([[ 0.1218, -0.2923,  0.7307],\n",
            "        [ 0.7740,  0.2903, -0.6773],\n",
            "        [ 0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0548,  0.0206, -0.0480],\n",
            "        [-0.1306, -0.0490,  0.1142]])\n",
            "hidden.bias.grad: tensor([ 0.2436,  0.9676,  0.0000,  0.0685, -0.1632])\n",
            "output.weight.grad: tensor([[-0.3551, -1.5191,  0.0000, -0.5498, -1.2630]])\n",
            "output.bias.grad: tensor([-3.8708])\n",
            "After hidden layer (raw output): tensor([[ 0.2334, -0.4832, -0.6943, -1.1832, -0.5430],\n",
            "        [-0.6343,  0.5826, -0.6934,  0.2171,  0.5059]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.2334, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5826, 0.0000, 0.2171, 0.5059]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[-0.3114],\n",
            "        [-0.4511]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Predicted output after one optimization step:\n",
            "tensor([[-0.3114],\n",
            "        [-0.4511]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss after one optimization step:\n",
            "tensor(3.8638, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - `loss = criterion(prediction, target)`: The loss function compares the predicted output with the true target values.\n",
        "  - `loss.backward()`: Backward propagation computes the gradients of the loss with respect to the model parameters.\n",
        "  - `optimizer.step()`: The optimizer updates the model's weights based on the computed gradients.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "EpSpCkJJ8O42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.4. Loss Functions and Optimizers**\n",
        "- **Loss Function**: Measures how well the model's predictions match the true target values.\n",
        "  - In regression, **Mean Squared Error** (`MSELoss`) is commonly used.\n",
        "  - In classification tasks, **Cross-Entropy Loss** (`nn.CrossEntropyLoss()`) is often used.\n",
        "- **Optimizer**: Updates the model’s parameters to minimize the loss using optimization algorithms like Stochastic Gradient Descent (SGD) or Adam.\n",
        "Here’s an extended list of **common loss functions** and **optimizers** in PyTorch, along with additional entries that are frequently used:\n",
        "\n",
        "\n",
        "- **Common Loss Functions in PyTorch**:\n",
        "  - `nn.MSELoss()`: For regression tasks, computes the mean squared error between predicted and target values.\n",
        "  - `nn.CrossEntropyLoss()`: For multi-class classification tasks, combines `nn.LogSoftmax()` and `nn.NLLLoss()`.\n",
        "  - `nn.BCELoss()`: For binary classification tasks, computes the binary cross-entropy loss between predicted probabilities and binary labels.\n",
        "  - `nn.BCEWithLogitsLoss()`: Combines a sigmoid layer with binary cross-entropy loss, making it more stable for binary classification.\n",
        "  - `nn.L1Loss()`: Computes the mean absolute error between predicted and target values. Useful when you want the loss to be less sensitive to outliers.\n",
        "  - `nn.HingeEmbeddingLoss()`: Used for binary classification, particularly in margin-based learning tasks like SVMs.\n",
        "  - `nn.MarginRankingLoss()`: Used for ranking tasks, especially for learning to rank problems.\n",
        "  - `nn.KLDivLoss()`: Kullback-Leibler divergence loss, often used for measuring the difference between two probability distributions.\n",
        "  - `nn.SmoothL1Loss()`: Also known as Huber loss, it is a combination of L1 and L2 losses, robust to outliers and used in regression tasks.\n",
        "  - `nn.NLLLoss()`: Negative Log-Likelihood Loss, often used with `nn.LogSoftmax()` for classification tasks where probabilities are log-scaled.\n",
        "\n",
        "- **Common Optimizers in PyTorch**:\n",
        "  - `optim.SGD()`: Stochastic Gradient Descent, a simple and widely used optimizer.\n",
        "  - `optim.Adam()`: Adam optimizer, combines momentum and adaptive learning rates, widely used for many tasks due to its fast convergence.\n",
        "  - `optim.AdamW()`: A variant of Adam with weight decay regularization for better generalization, especially useful in deep learning tasks.\n",
        "  - `optim.RMSprop()`: An adaptive learning rate method that adjusts the learning rate based on recent gradients, often used in recurrent neural networks (RNNs).\n",
        "  - `optim.Adagrad()`: Adaptive Gradient Algorithm, adjusts learning rates dynamically for each parameter based on the sum of squared gradients.\n",
        "  - `optim.Adadelta()`: An extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rates over time.\n",
        "  - `optim.ASGD()`: Averaged Stochastic Gradient Descent, averages the weights over time to achieve better generalization.\n",
        "  - `optim.LBFGS()`: Limited-memory Broyden–Fletcher–Goldfarb–Shanno, a quasi-Newton optimization method, used for smaller datasets where second-order methods can be effective.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "uoVblLfT8O8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.5. Training a Neural Network**\n",
        "- The process of training a neural network involves multiple iterations (epochs) of forward and backward propagation.\n",
        "- At each epoch:\n",
        "  - The model makes predictions (forward pass).\n",
        "  - The loss is calculated based on the predictions and actual targets.\n",
        "  - Gradients are computed using backpropagation.\n",
        "  - The optimizer updates the weights based on the gradients.\n",
        "\n",
        "**2.5.1. Full Training Loop**\n",
        "\n",
        "  **Demonstration: Training a Simple Neural Network**"
      ],
      "metadata": {
        "id": "_UUUAYUT8O_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function (criterion), and optimizer\n",
        "# Assuming the model is already defined as `SimpleNN`\n",
        "# `nn.MSELoss()` is used for regression tasks to calculate the mean squared error\n",
        "# `optim.SGD()` is the optimizer used to update model weights using stochastic gradient descent\n",
        "# These components are necessary for training a neural network.\n",
        "\n",
        "# model = SimpleNN()  # Already defined model\n",
        "# criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01)  # SGD optimizer with learning rate of 0.01\n",
        "\n",
        "# Training data: A batch of inputs (2 samples, each with 3 features)\n",
        "# This tensor represents two samples with 3 features each. 'requires_grad=True' ensures\n",
        "# that we can calculate gradients with respect to this input during training.\n",
        "input_data = torch.tensor([[0.5, -1.2, 3.0], [0.8, 0.3, -0.7]], requires_grad=True)\n",
        "\n",
        "# Corresponding target values for regression.\n",
        "# These are the expected (target) output values that the model should predict during training.\n",
        "target = torch.tensor([[1.0], [0.5]])\n",
        "\n",
        "# Set the number of epochs (iterations).\n",
        "# This specifies how many times the training loop will run, updating the model weights after each iteration.\n",
        "epochs = 10\n",
        "\n",
        "# Training loop: This loop will train the model for a specified number of epochs (iterations).\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")  # Print the current epoch number for tracking progress\n",
        "\n",
        "    # Forward pass: Make predictions based on the current state of the model.\n",
        "    # Here, the input_data is passed through the model to compute the predictions.\n",
        "    prediction = model(input_data)\n",
        "    print(f\"Predicted output:\\n{prediction}\")  # Display the predictions for this epoch\n",
        "\n",
        "    # Calculate the loss: This measures how far off the model's predictions are from the target values.\n",
        "    # The loss function compares the predicted values with the actual target values.\n",
        "    loss = criterion(prediction, target)\n",
        "    print(f\"Loss: {loss.item():.4f}\")  # Print the current loss value (error)\n",
        "\n",
        "    # Zero the gradients: Before the backward pass, we need to clear the gradients from the previous iteration.\n",
        "    # Gradients accumulate by default, so we must zero them out before the next iteration.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: Calculate the gradients of the loss with respect to the model's parameters.\n",
        "    # This step computes how each parameter (weights and biases) should change to minimize the loss.\n",
        "    loss.backward()\n",
        "\n",
        "    # Print gradients after the backward pass.\n",
        "    # We loop through each parameter and print its gradient values. These gradients show the direction\n",
        "    # and magnitude of change needed for each weight to reduce the loss.\n",
        "    print(\"Gradients after backward pass:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            print(f\"{name}.grad:\\n{param.grad}\")  # Print gradients of weights and biases\n",
        "\n",
        "    # Update model weights: The optimizer updates the model's weights using the computed gradients.\n",
        "    # This step modifies the parameters based on the gradients and the learning rate (0.01 in this case).\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print updated model parameters (weights and biases).\n",
        "    # After each optimization step, we print the new parameter values to see how they change after every epoch.\n",
        "    print(\"Updated model parameters:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name}:\\n{param}\")\n",
        "\n",
        "    # Print a separator line for better readability between epochs.\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "fyv-5sn-8O_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9864dc20-3f57-4634-e49b-6ca6f8791c7c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "After hidden layer (raw output): tensor([[-0.0201, -0.6019, -0.6943, -1.1961, -0.5384],\n",
            "        [-0.6113,  0.8312, -0.6934,  0.2441,  0.4963]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8312, 0.0000, 0.2441, 0.4963]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[0.9956],\n",
            "        [0.5021]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[0.9956],\n",
            "        [0.5021]], grad_fn=<AddmmBackward0>)\n",
            "Loss: 0.0000\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.9988e-04, -3.3746e-04,  7.8740e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.3014e-04, -4.8803e-05,  1.1387e-04],\n",
            "        [-7.6445e-05, -2.8667e-05,  6.6890e-05]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000e+00, -1.1249e-03,  0.0000e+00, -1.6268e-04, -9.5557e-05])\n",
            "output.weight.grad:\n",
            "tensor([[0.0000, 0.0017, 0.0000, 0.0005, 0.0010]])\n",
            "output.bias.grad:\n",
            "tensor([-0.0023])\n",
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2675,  0.5687,  0.3687],\n",
            "        [ 0.2916,  0.2603, -0.2582],\n",
            "        [-0.1822,  0.3441,  0.1245],\n",
            "        [ 0.3390,  0.3109, -0.2357],\n",
            "        [-0.1305, -0.2243, -0.3812]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3099,  0.3392, -0.5638, -0.2853,  0.4012], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.1609, -0.5431,  0.1203, -0.0785, -0.0461]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([0.9956], requires_grad=True)\n",
            "--------------------------------------------------\n",
            "Epoch 2/10\n",
            "After hidden layer (raw output): tensor([[-0.0201, -0.6019, -0.6943, -1.1961, -0.5384],\n",
            "        [-0.6113,  0.8312, -0.6934,  0.2441,  0.4963]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8312, 0.0000, 0.2441, 0.4963]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[0.9956],\n",
            "        [0.5021]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[0.9956],\n",
            "        [0.5021]], grad_fn=<AddmmBackward0>)\n",
            "Loss: 0.0000\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.9512e-04, -3.3567e-04,  7.8323e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.2946e-04, -4.8546e-05,  1.1327e-04],\n",
            "        [-7.6055e-05, -2.8521e-05,  6.6548e-05]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000e+00, -1.1189e-03,  0.0000e+00, -1.6182e-04, -9.5069e-05])\n",
            "output.weight.grad:\n",
            "tensor([[0.0000, 0.0017, 0.0000, 0.0005, 0.0010]])\n",
            "output.bias.grad:\n",
            "tensor([-0.0023])\n",
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2675,  0.5687,  0.3687],\n",
            "        [ 0.2916,  0.2603, -0.2582],\n",
            "        [-0.1822,  0.3441,  0.1245],\n",
            "        [ 0.3390,  0.3109, -0.2357],\n",
            "        [-0.1305, -0.2243, -0.3812]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3099,  0.3392, -0.5638, -0.2853,  0.4012], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.1609, -0.5431,  0.1203, -0.0786, -0.0462]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([0.9956], requires_grad=True)\n",
            "--------------------------------------------------\n",
            "Epoch 3/10\n",
            "After hidden layer (raw output): tensor([[-0.0201, -0.6019, -0.6943, -1.1961, -0.5384],\n",
            "        [-0.6113,  0.8313, -0.6934,  0.2441,  0.4963]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8313, 0.0000, 0.2441, 0.4963]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[0.9956],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[0.9956],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Loss: 0.0000\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.9036e-04, -3.3388e-04,  7.7906e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.2877e-04, -4.8289e-05,  1.1267e-04],\n",
            "        [-7.5665e-05, -2.8374e-05,  6.6207e-05]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000e+00, -1.1129e-03,  0.0000e+00, -1.6096e-04, -9.4581e-05])\n",
            "output.weight.grad:\n",
            "tensor([[0.0000, 0.0017, 0.0000, 0.0005, 0.0010]])\n",
            "output.bias.grad:\n",
            "tensor([-0.0023])\n",
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2675,  0.5687,  0.3687],\n",
            "        [ 0.2916,  0.2603, -0.2582],\n",
            "        [-0.1822,  0.3441,  0.1245],\n",
            "        [ 0.3390,  0.3109, -0.2357],\n",
            "        [-0.1305, -0.2243, -0.3812]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3099,  0.3392, -0.5638, -0.2853,  0.4012], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.1609, -0.5432,  0.1203, -0.0786, -0.0462]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([0.9956], requires_grad=True)\n",
            "--------------------------------------------------\n",
            "Epoch 4/10\n",
            "After hidden layer (raw output): tensor([[-0.0201, -0.6019, -0.6943, -1.1961, -0.5384],\n",
            "        [-0.6113,  0.8313, -0.6934,  0.2441,  0.4963]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8313, 0.0000, 0.2441, 0.4963]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[0.9956],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[0.9956],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Loss: 0.0000\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.8562e-04, -3.3211e-04,  7.7492e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.2809e-04, -4.8034e-05,  1.1208e-04],\n",
            "        [-7.5276e-05, -2.8229e-05,  6.5867e-05]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000e+00, -1.1070e-03,  0.0000e+00, -1.6011e-04, -9.4096e-05])\n",
            "output.weight.grad:\n",
            "tensor([[0.0000, 0.0017, 0.0000, 0.0005, 0.0010]])\n",
            "output.bias.grad:\n",
            "tensor([-0.0023])\n",
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2675,  0.5687,  0.3687],\n",
            "        [ 0.2916,  0.2603, -0.2582],\n",
            "        [-0.1822,  0.3441,  0.1245],\n",
            "        [ 0.3390,  0.3109, -0.2357],\n",
            "        [-0.1305, -0.2243, -0.3812]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3099,  0.3392, -0.5638, -0.2853,  0.4012], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.1609, -0.5432,  0.1203, -0.0786, -0.0462]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([0.9957], requires_grad=True)\n",
            "--------------------------------------------------\n",
            "Epoch 5/10\n",
            "After hidden layer (raw output): tensor([[-0.0201, -0.6019, -0.6943, -1.1961, -0.5384],\n",
            "        [-0.6113,  0.8313, -0.6934,  0.2441,  0.4963]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8313, 0.0000, 0.2441, 0.4963]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[0.9957],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[0.9957],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Loss: 0.0000\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.8091e-04, -3.3034e-04,  7.7079e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.2741e-04, -4.7780e-05,  1.1149e-04],\n",
            "        [-7.4890e-05, -2.8084e-05,  6.5529e-05]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000e+00, -1.1011e-03,  0.0000e+00, -1.5927e-04, -9.3613e-05])\n",
            "output.weight.grad:\n",
            "tensor([[0.0000, 0.0017, 0.0000, 0.0005, 0.0010]])\n",
            "output.bias.grad:\n",
            "tensor([-0.0023])\n",
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2675,  0.5687,  0.3687],\n",
            "        [ 0.2916,  0.2603, -0.2582],\n",
            "        [-0.1822,  0.3441,  0.1245],\n",
            "        [ 0.3390,  0.3109, -0.2357],\n",
            "        [-0.1305, -0.2243, -0.3812]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3099,  0.3392, -0.5638, -0.2853,  0.4012], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.1609, -0.5432,  0.1203, -0.0786, -0.0462]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([0.9957], requires_grad=True)\n",
            "--------------------------------------------------\n",
            "Epoch 6/10\n",
            "After hidden layer (raw output): tensor([[-0.0201, -0.6019, -0.6943, -1.1961, -0.5384],\n",
            "        [-0.6113,  0.8313, -0.6934,  0.2441,  0.4963]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8313, 0.0000, 0.2441, 0.4963]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[0.9957],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[0.9957],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Loss: 0.0000\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.7622e-04, -3.2858e-04,  7.6669e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.2674e-04, -4.7527e-05,  1.1090e-04],\n",
            "        [-7.4505e-05, -2.7940e-05,  6.5192e-05]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000e+00, -1.0953e-03,  0.0000e+00, -1.5842e-04, -9.3132e-05])\n",
            "output.weight.grad:\n",
            "tensor([[0.0000, 0.0017, 0.0000, 0.0005, 0.0010]])\n",
            "output.bias.grad:\n",
            "tensor([-0.0023])\n",
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2675,  0.5687,  0.3687],\n",
            "        [ 0.2916,  0.2603, -0.2582],\n",
            "        [-0.1822,  0.3441,  0.1245],\n",
            "        [ 0.3390,  0.3109, -0.2357],\n",
            "        [-0.1305, -0.2243, -0.3812]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3099,  0.3392, -0.5638, -0.2853,  0.4012], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.1609, -0.5432,  0.1203, -0.0786, -0.0462]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([0.9957], requires_grad=True)\n",
            "--------------------------------------------------\n",
            "Epoch 7/10\n",
            "After hidden layer (raw output): tensor([[-0.0201, -0.6019, -0.6943, -1.1961, -0.5384],\n",
            "        [-0.6113,  0.8314, -0.6934,  0.2441,  0.4963]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8314, 0.0000, 0.2441, 0.4963]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[0.9957],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[0.9957],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Loss: 0.0000\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.7156e-04, -3.2684e-04,  7.6262e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.2607e-04, -4.7276e-05,  1.1031e-04],\n",
            "        [-7.4123e-05, -2.7796e-05,  6.4858e-05]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000e+00, -1.0895e-03,  0.0000e+00, -1.5759e-04, -9.2654e-05])\n",
            "output.weight.grad:\n",
            "tensor([[0.0000, 0.0017, 0.0000, 0.0005, 0.0010]])\n",
            "output.bias.grad:\n",
            "tensor([-0.0023])\n",
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2675,  0.5687,  0.3687],\n",
            "        [ 0.2917,  0.2603, -0.2582],\n",
            "        [-0.1822,  0.3441,  0.1245],\n",
            "        [ 0.3390,  0.3109, -0.2357],\n",
            "        [-0.1305, -0.2243, -0.3812]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3099,  0.3392, -0.5638, -0.2853,  0.4012], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.1609, -0.5432,  0.1203, -0.0786, -0.0462]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([0.9957], requires_grad=True)\n",
            "--------------------------------------------------\n",
            "Epoch 8/10\n",
            "After hidden layer (raw output): tensor([[-0.0201, -0.6020, -0.6943, -1.1961, -0.5385],\n",
            "        [-0.6113,  0.8314, -0.6934,  0.2441,  0.4964]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8314, 0.0000, 0.2441, 0.4964]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[0.9957],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[0.9957],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Loss: 0.0000\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.6690e-04, -3.2509e-04,  7.5854e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.2540e-04, -4.7024e-05,  1.0972e-04],\n",
            "        [-7.3740e-05, -2.7653e-05,  6.4523e-05]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000e+00, -1.0836e-03,  0.0000e+00, -1.5675e-04, -9.2175e-05])\n",
            "output.weight.grad:\n",
            "tensor([[0.0000, 0.0017, 0.0000, 0.0005, 0.0010]])\n",
            "output.bias.grad:\n",
            "tensor([-0.0023])\n",
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2675,  0.5687,  0.3687],\n",
            "        [ 0.2917,  0.2603, -0.2582],\n",
            "        [-0.1822,  0.3441,  0.1245],\n",
            "        [ 0.3390,  0.3109, -0.2357],\n",
            "        [-0.1304, -0.2243, -0.3812]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3099,  0.3392, -0.5638, -0.2853,  0.4012], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.1609, -0.5432,  0.1203, -0.0786, -0.0462]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([0.9958], requires_grad=True)\n",
            "--------------------------------------------------\n",
            "Epoch 9/10\n",
            "After hidden layer (raw output): tensor([[-0.0201, -0.6020, -0.6943, -1.1961, -0.5385],\n",
            "        [-0.6113,  0.8314, -0.6934,  0.2441,  0.4964]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8314, 0.0000, 0.2441, 0.4964]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[0.9958],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[0.9958],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Loss: 0.0000\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.6229e-04, -3.2336e-04,  7.5450e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.2474e-04, -4.6776e-05,  1.0914e-04],\n",
            "        [-7.3361e-05, -2.7511e-05,  6.4191e-05]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000e+00, -1.0779e-03,  0.0000e+00, -1.5592e-04, -9.1702e-05])\n",
            "output.weight.grad:\n",
            "tensor([[0.0000, 0.0016, 0.0000, 0.0005, 0.0010]])\n",
            "output.bias.grad:\n",
            "tensor([-0.0023])\n",
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2675,  0.5687,  0.3687],\n",
            "        [ 0.2917,  0.2603, -0.2582],\n",
            "        [-0.1822,  0.3441,  0.1245],\n",
            "        [ 0.3390,  0.3109, -0.2357],\n",
            "        [-0.1304, -0.2243, -0.3812]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3099,  0.3392, -0.5638, -0.2853,  0.4012], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.1609, -0.5433,  0.1203, -0.0786, -0.0462]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([0.9958], requires_grad=True)\n",
            "--------------------------------------------------\n",
            "Epoch 10/10\n",
            "After hidden layer (raw output): tensor([[-0.0201, -0.6020, -0.6943, -1.1961, -0.5385],\n",
            "        [-0.6113,  0.8314, -0.6934,  0.2442,  0.4964]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "After ReLU activation: tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8314, 0.0000, 0.2442, 0.4964]], grad_fn=<ReluBackward0>)\n",
            "Final output (before returning): tensor([[0.9958],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Predicted output:\n",
            "tensor([[0.9958],\n",
            "        [0.5020]], grad_fn=<AddmmBackward0>)\n",
            "Loss: 0.0000\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-8.5770e-04, -3.2164e-04,  7.5049e-04],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [-1.2408e-04, -4.6528e-05,  1.0857e-04],\n",
            "        [-7.2985e-05, -2.7369e-05,  6.3862e-05]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000e+00, -1.0721e-03,  0.0000e+00, -1.5509e-04, -9.1231e-05])\n",
            "output.weight.grad:\n",
            "tensor([[0.0000, 0.0016, 0.0000, 0.0005, 0.0010]])\n",
            "output.bias.grad:\n",
            "tensor([-0.0022])\n",
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2675,  0.5687,  0.3687],\n",
            "        [ 0.2917,  0.2603, -0.2582],\n",
            "        [-0.1822,  0.3441,  0.1245],\n",
            "        [ 0.3390,  0.3109, -0.2357],\n",
            "        [-0.1304, -0.2243, -0.3812]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3099,  0.3392, -0.5638, -0.2853,  0.4012], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.1609, -0.5433,  0.1203, -0.0786, -0.0462]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([0.9958], requires_grad=True)\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - The model is trained for 10 epochs.\n",
        "  - At each epoch, a forward pass is performed to predict the output based on the current weights.\n",
        "  - The loss is calculated, gradients are computed via backpropagation, and weights are updated using the optimizer.\n",
        "  - The loss is printed every 100 epochs to monitor the training process.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9XCqUFJv8PBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.6. Observations on State-of-the-Art Research**:\n",
        "  - **Gradient-based Optimization**: Current research is focused on improving optimization techniques to deal with issues like vanishing/exploding gradients and slow convergence.\n",
        "    - **AdamW Optimizer**: A variant of the Adam optimizer, `AdamW`, addresses weight decay regularization and has become a standard for many models.\n",
        "    - **Gradient Clipping**: Clipping gradients to prevent them from becoming too large has become common practice in deep learning to avoid instability.\n",
        "  \n",
        "  - **Activation Functions**:\n",
        "    - While ReLU remains widely used due to its simplicity, newer activation functions like **Leaky ReLU**, **PReLU**, and **Swish** have been shown to provide better performance in some architectures by mitigating issues like the \"dying ReLU\" problem.\n",
        "\n",
        "  - **Batch Normalization**:\n",
        "    - Batch normalization has become an essential component in training deep networks. It helps stabilize training by normalizing inputs to layers, allowing for higher learning rates and faster convergence.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PnxWm2n_8PF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **2.7. Adding Continuity to the Next Section**\n",
        "- In the next section, we will explore how to efficiently manage datasets and dataloaders using PyTorch's `torch.utils.data` module.\n",
        "- We will load and preprocess datasets, including commonly used datasets like MNIST and CIFAR-\n",
        "\n",
        "10, and handle custom datasets.\n",
        "  \n",
        "This section introduced you to building simple neural networks, defining layers, performing forward and backward propagation, and training the model with different loss functions and optimizers. You can now move on to handling datasets for larger and more complex neural network training tasks."
      ],
      "metadata": {
        "id": "ichC1eAi8PJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations"
      ],
      "metadata": {
        "id": "endC4SUVG_bN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Question 3: **How are layers connected in a PyTorch neural network?**\n",
        "\n",
        "##### Answer:\n",
        "In PyTorch, layers are typically connected in the `forward()` method of the neural network class, which defines how data flows through the network:\n",
        "\n",
        "- Each layer is created in the `__init__()` method and saved as part of the class. These layers are connected by passing the output of one layer as input to the next.\n",
        "- The final layer output gives the prediction of the network.\n",
        "\n",
        "##### Example:\n"
      ],
      "metadata": {
        "id": "itV7-HE8G-9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple neural network class by inheriting from nn.Module\n",
        "class SimpleNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Call the constructor of nn.Module to initialize the base class\n",
        "        super(SimpleNeuralNet, self).__init__()\n",
        "\n",
        "        # Define the hidden layer\n",
        "        # nn.Linear is a fully connected layer that applies a linear transformation\n",
        "        # This layer takes 'input_size' features and maps them to 'hidden_size' outputs (neurons)\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Define the output layer\n",
        "        # This layer takes 'hidden_size' inputs from the hidden layer and maps them to 'output_size' outputs\n",
        "        # Typically, 'output_size' is the number of predictions you want to make (e.g., for regression or classification)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    # Define the forward pass, which describes how data moves through the network\n",
        "    def forward(self, x):\n",
        "        # Pass the input 'x' through the hidden layer\n",
        "        # This step applies a linear transformation: y = xW + b, where W is the weight matrix and b is the bias\n",
        "        x = self.hidden(x)\n",
        "\n",
        "        # Apply the ReLU activation function to introduce non-linearity\n",
        "        # ReLU sets any negative values to zero, helping the model learn more complex patterns\n",
        "        x = torch.relu(x)\n",
        "\n",
        "        # Pass the result through the output layer to produce the final output\n",
        "        # No activation function is applied here, as this is a simple regression model or an unactivated output\n",
        "        x = self.output(x)\n",
        "\n",
        "        # Return the final output of the network\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "w_SEZIY6Orqd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Question 4: **What is the role of activation functions in PyTorch?**\n",
        "\n",
        "##### Answer:\n",
        "Activation functions introduce non-linearity into the neural network, allowing it to learn complex patterns. Without activation functions, the model would just be a linear combination of inputs.\n",
        "\n",
        "Common activation functions:\n",
        "- **ReLU (Rectified Linear Unit)**: Used in hidden layers, it outputs 0 for negative inputs and the input itself for positive inputs.\n",
        "- **Sigmoid**: Used in binary classification to output probabilities between 0 and 1.\n",
        "- **Softmax**: Used in multi-class classification to output probabilities for each class.\n",
        "\n",
        "##### Example:\n"
      ],
      "metadata": {
        "id": "2zdcFviiOxZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple neural network class that inherits from nn.Module\n",
        "class SimpleNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNeuralNet, self).__init__()\n",
        "\n",
        "        # Step 1: Define the hidden layer\n",
        "        # The hidden layer is a fully connected (dense) layer that takes 'input_size' features\n",
        "        # and outputs 'hidden_size' neurons.\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Step 2: Define the output layer\n",
        "        # The output layer is a fully connected layer that takes 'hidden_size' outputs from the hidden layer\n",
        "        # and produces 'output_size' outputs. These could be the class probabilities (for classification) or\n",
        "        # raw values (for regression).\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Step 3: Define the activation functions\n",
        "        # ReLU (Rectified Linear Unit) is used for the hidden layer to introduce non-linearity.\n",
        "        # It replaces negative values with zero and keeps positive values unchanged.\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Softmax is used for the output layer, typically in classification tasks.\n",
        "        # Softmax converts the output scores into probabilities that sum to 1 across the given dimension.\n",
        "        # 'dim=1' means the softmax will be applied across the second dimension (rows) of the output tensor.\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    # Define the forward pass, which describes how the input data flows through the layers\n",
        "    def forward(self, x):\n",
        "        # Step 4: Pass the input through the hidden layer\n",
        "        x = self.hidden(x)  # Linear transformation: xW + b\n",
        "\n",
        "        # Step 5: Apply the ReLU activation function to the hidden layer output\n",
        "        x = self.relu(x)  # Apply ReLU non-linearity\n",
        "\n",
        "        # Step 6: Pass the result through the output layer\n",
        "        x = self.output(x)  # Linear transformation: xW' + b'\n",
        "\n",
        "        # Step 7: Apply the Softmax activation function to the output layer's result\n",
        "        # Softmax is often used in classification to output probabilities for each class.\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        # Step 8: Return the final output of the network\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "LyFqsoDtO0Xg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Question 5: **How are weights initialized in PyTorch layers?**\n",
        "\n",
        "##### Answer:\n",
        "In PyTorch, weights and biases for each layer are initialized automatically when you create a layer. However, you can also manually initialize weights using custom methods (e.g., `torch.nn.init` methods).\n",
        "\n",
        "- **Default Initialization**: By default, PyTorch uses uniform or normal distributions for initializing weights depending on the layer.\n",
        "  \n",
        "- **Custom Initialization**: You can manually initialize weights using functions like `nn.init.xavier_uniform_()` or `nn.init.kaiming_uniform_()`.\n",
        "\n",
        "##### Example:\n"
      ],
      "metadata": {
        "id": "GzefLWiZO0tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init  # Import initialization methods\n",
        "\n",
        "# Define a simple neural network class\n",
        "class SimpleNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        # Initialize the nn.Module parent class\n",
        "        super(SimpleNeuralNet, self).__init__()\n",
        "\n",
        "        # Define the hidden and output layers\n",
        "        # Hidden layer: Maps 'input_size' features to 'hidden_size' neurons\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Output layer: Maps 'hidden_size' neurons to 'output_size' outputs\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Define ReLU activation function for the hidden layer\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Step 1: Custom weight initialization for the hidden layer\n",
        "        # Xavier initialization is often used for layers with ReLU activation\n",
        "        # It helps the network start with weights that have a good scale for training\n",
        "        init.xavier_uniform_(self.hidden.weight)\n",
        "\n",
        "        # Step 2: Initialize the biases of the output layer to zero\n",
        "        # This ensures that the initial bias term starts at zero\n",
        "        init.zeros_(self.output.bias)\n",
        "\n",
        "    # Define the forward pass through the network\n",
        "    def forward(self, x):\n",
        "        # Step 3: Pass input through the hidden layer\n",
        "        x = self.hidden(x)\n",
        "\n",
        "        # Step 4: Apply ReLU activation to the hidden layer output\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Step 5: Pass the result through the output layer\n",
        "        x = self.output(x)\n",
        "\n",
        "        # Step 6: Return the final output\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "GrUAoxZHO1JR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Question 6: **What are loss functions, and how are they used in PyTorch?**\n",
        "\n",
        "##### Answer:\n",
        "Loss functions measure the difference between the predicted output and the true labels. During training, the network tries to minimize this loss using optimization algorithms.\n",
        "\n",
        "Common loss functions:\n",
        "- **Mean Squared Error (MSE)**: Used for regression tasks.\n",
        "- **Cross-Entropy Loss**: Used for classification tasks (binary or multi-class).\n",
        "\n",
        "##### Example:\n"
      ],
      "metadata": {
        "id": "ELibJrE_O1g3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple neural network with one hidden layer\n",
        "class SimpleNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNeuralNet, self).__init__()\n",
        "\n",
        "        # Define the hidden layer (fully connected layer)\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Define the output layer (fully connected layer)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    # Define the forward pass through the network\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the hidden layer\n",
        "        print(f\"Input to hidden layer:\\n{x}\")\n",
        "        x = self.hidden(x)\n",
        "        print(f\"Output of hidden layer (before activation):\\n{x}\")\n",
        "\n",
        "        # Apply the ReLU activation function to introduce non-linearity\n",
        "        x = torch.relu(x)\n",
        "        print(f\"Output of hidden layer (after ReLU activation):\\n{x}\")\n",
        "\n",
        "        # Pass through the output layer (no activation function at this stage because\n",
        "        # we'll use CrossEntropyLoss, which includes Softmax internally)\n",
        "        x = self.output(x)\n",
        "        print(f\"Output of final layer (logits, before softmax):\\n{x}\")\n",
        "\n",
        "        # Return the final output (logits)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model with:\n",
        "# - 10 input features\n",
        "# - 5 neurons in the hidden layer\n",
        "# - 3 output neurons (corresponding to 3 classes for classification)\n",
        "model = SimpleNeuralNet(input_size=10, hidden_size=5, output_size=3)\n",
        "print(\"Initialized model:\\n\", model)\n",
        "\n",
        "# Define the loss function (Cross-Entropy Loss) for classification\n",
        "# CrossEntropyLoss combines LogSoftmax and NLLLoss, which is suitable for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Generate a random input tensor representing one sample with 10 features\n",
        "input_data = torch.randn(1, 10)  # Perform a forward pass through the model\n",
        "print(f\"\\nRandom input data (1 sample, 10 features):\\n{input_data}\")\n",
        "\n",
        "# Forward pass: Get the model's output (logits) for this random input\n",
        "output = model(input_data)\n",
        "\n",
        "# Define the target as class 1 (the ground truth for the classification task)\n",
        "# Note that target should contain class indices, not one-hot encoded vectors\n",
        "target = torch.tensor([1])  # This means the correct class for the input is class 1\n",
        "print(f\"\\nTarget class index:\\n{target}\")\n",
        "\n",
        "# Calculate the loss between the model's prediction (logits) and the true target\n",
        "loss = criterion(output, target)\n",
        "print(f\"\\nLoss value (Cross-Entropy Loss):\\n{loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1NMPhQSPGRz",
        "outputId": "214e348e-4103-46d9-c33f-4c789dbad089"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized model:\n",
            " SimpleNeuralNet(\n",
            "  (hidden): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (output): Linear(in_features=5, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[-0.2935,  0.4157, -1.0423,  0.3393, -1.1876, -0.3527, -1.6055, -0.4963,\n",
            "          0.6685,  0.7962]])\n",
            "Input to hidden layer:\n",
            "tensor([[-0.2935,  0.4157, -1.0423,  0.3393, -1.1876, -0.3527, -1.6055, -0.4963,\n",
            "          0.6685,  0.7962]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[-0.6597,  0.1025, -0.1949, -0.9492, -0.4829]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.0000, 0.1025, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[-0.3365, -0.0719, -0.3390]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Target class index:\n",
            "tensor([1])\n",
            "\n",
            "Loss value (Cross-Entropy Loss):\n",
            "0.9294456839561462\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Steps after Calculating the Loss:\n",
        "\n",
        "- Zero the Gradients:\n",
        "  - Gradients accumulate by default in PyTorch, so you must clear the old gradients before computing new ones. This is done using optimizer.zero_grad().\n",
        "\n",
        "- Backpropagation:\n",
        "  - Perform backward propagation to compute the gradients of the loss with respect to the model’s parameters (weights and biases). This is done using loss.backward(), which calculates the gradients for each parameter.\n",
        "\n",
        "- Optimizer Step:\n",
        "  - Once the gradients are computed, you use the optimizer to update the model parameters. This step adjusts the weights and biases of the network to minimize the loss. This is done using optimizer.step()."
      ],
      "metadata": {
        "id": "EAreTRZ-ZYpF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Zero the gradients from the previous backward pass\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Step 2: Perform backpropagation to compute the gradients\n",
        "loss.backward()\n",
        "\n",
        "# Step 3: Update the model parameters using the optimizer\n",
        "optimizer.step()\n",
        "\n",
        "# Print the updated model parameters after optimization\n",
        "print(\"Updated model parameters:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}:\\n{param}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9UAtfwNZgYU",
        "outputId": "e3c6fd54-791a-4119-ea4d-c5a533bdfcf4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated model parameters:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[ 0.2768,  0.0388, -0.2583,  0.2788,  0.1130,  0.1815,  0.0303,  0.1883,\n",
            "         -0.2732, -0.1995],\n",
            "        [ 0.0697,  0.1735, -0.1501,  0.0195, -0.2969, -0.1825,  0.1080,  0.2503,\n",
            "         -0.2624,  0.1561],\n",
            "        [ 0.2598, -0.0559,  0.1124, -0.2343, -0.1168,  0.2740, -0.2217, -0.1452,\n",
            "         -0.0128, -0.1145],\n",
            "        [ 0.3030,  0.2274,  0.1841, -0.2281,  0.0573,  0.1298,  0.1362,  0.2095,\n",
            "         -0.0574, -0.2441],\n",
            "        [-0.0162,  0.2711, -0.0591, -0.2672, -0.1973, -0.0737,  0.2943, -0.3159,\n",
            "         -0.2851, -0.1880]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.2767, -0.1804, -0.2691, -0.0163, -0.1756], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.3330, -0.0419, -0.2449,  0.2733,  0.3738],\n",
            "        [-0.3713,  0.2972, -0.0493,  0.3442,  0.4066],\n",
            "        [ 0.3321,  0.3652, -0.2726, -0.3417, -0.1864]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.3322, -0.1024, -0.3764], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Note: Then next, repeat the whole step until we find the efficient model"
      ],
      "metadata": {
        "id": "Go-9mIhJZziz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Question 7: **How are optimizers used in PyTorch?**\n",
        "\n",
        "##### Answer:\n",
        "Optimizers update the weights of the model during training based on the gradients of the loss function. The most commonly used optimizer is **Stochastic Gradient Descent (SGD)**, but **Adam** is also popular for its adaptive learning rates.\n",
        "\n",
        "##### Example:\n"
      ],
      "metadata": {
        "id": "A470i7_ZPH2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNeuralNet, self).__init__()\n",
        "\n",
        "        # Define the hidden layer (fully connected layer)\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Define the output layer (fully connected layer)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    # Define the forward pass through the network\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the hidden layer\n",
        "        print(f\"Input to hidden layer:\\n{x}\")\n",
        "        x = self.hidden(x)\n",
        "        print(f\"Output of hidden layer (before activation):\\n{x}\")\n",
        "\n",
        "        # Apply the ReLU activation function to introduce non-linearity\n",
        "        x = torch.relu(x)\n",
        "        print(f\"Output of hidden layer (after ReLU activation):\\n{x}\")\n",
        "\n",
        "        # Pass through the output layer (no activation here, raw logits)\n",
        "        x = self.output(x)\n",
        "        print(f\"Output of final layer (logits, before softmax):\\n{x}\")\n",
        "\n",
        "        # Return the final output (logits)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model with:\n",
        "# - 10 input features\n",
        "# - 5 neurons in the hidden layer\n",
        "# - 3 output neurons (for classification into 3 classes)\n",
        "model = SimpleNeuralNet(input_size=10, hidden_size=5, output_size=3)\n",
        "print(\"Initialized model:\\n\", model)\n",
        "\n",
        "# Define an optimizer (Adam optimizer) to update the weights\n",
        "# The learning rate is set to 0.001\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the loss function (Cross-Entropy Loss) for classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Generate a random input tensor (1 sample, 10 features)\n",
        "input_data = torch.randn(1, 10)\n",
        "print(f\"\\nRandom input data (1 sample, 10 features):\\n{input_data}\")\n",
        "\n",
        "# Forward pass: Get the model's output (logits) for this random input\n",
        "output = model(input_data)\n",
        "\n",
        "# Define the target as class 1 (ground truth)\n",
        "target = torch.tensor([1])  # The correct class is 1\n",
        "print(f\"\\nTarget class index:\\n{target}\")\n",
        "\n",
        "# Calculate the loss between the model's output (logits) and the true target\n",
        "loss = criterion(output, target)\n",
        "print(f\"\\nLoss value (Cross-Entropy Loss):\\n{loss.item()}\")\n",
        "\n",
        "# Backward pass to compute gradients\n",
        "optimizer.zero_grad()  # Clear the gradients before backpropagation\n",
        "print(\"\\nGradients cleared.\")\n",
        "\n",
        "# Backpropagate the loss to compute gradients of the weights\n",
        "loss.backward()\n",
        "print(\"Gradients after backward pass:\")\n",
        "for name, param in model.named_parameters():\n",
        "    if param.grad is not None:\n",
        "        print(f\"{name}.grad:\\n{param.grad}\")\n",
        "\n",
        "# Perform the optimizer step to update model weights\n",
        "optimizer.step()\n",
        "print(\"\\nModel parameters updated after optimizer step:\")\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"{name}:\\n{param}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZHeUG8OPJRf",
        "outputId": "4b075ef0-db84-4292-c556-7c5032b86706"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized model:\n",
            " SimpleNeuralNet(\n",
            "  (hidden): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (output): Linear(in_features=5, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[-0.2007, -0.4387,  1.2775, -0.0624, -0.6971,  0.3294, -0.6417,  0.1808,\n",
            "         -1.4538,  0.2542]])\n",
            "Input to hidden layer:\n",
            "tensor([[-0.2007, -0.4387,  1.2775, -0.0624, -0.6971,  0.3294, -0.6417,  0.1808,\n",
            "         -1.4538,  0.2542]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[-1.1955,  0.5762,  0.3802,  0.0842,  0.3115]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.0000, 0.5762, 0.3802, 0.0842, 0.3115]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[-0.7532, -0.1636, -0.1574]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Target class index:\n",
            "tensor([1])\n",
            "\n",
            "Loss value (Cross-Entropy Loss):\n",
            "0.9403031468391418\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[-0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00, -0.0000e+00,\n",
            "          0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00],\n",
            "        [-5.7496e-02, -1.2564e-01,  3.6588e-01, -1.7884e-02, -1.9966e-01,\n",
            "          9.4354e-02, -1.8379e-01,  5.1795e-02, -4.1639e-01,  7.2818e-02],\n",
            "        [-1.1478e-02, -2.5082e-02,  7.3041e-02, -3.5703e-03, -3.9859e-02,\n",
            "          1.8836e-02, -3.6689e-02,  1.0340e-02, -8.3124e-02,  1.4537e-02],\n",
            "        [ 1.1327e-03,  2.4753e-03, -7.2083e-03,  3.5234e-04,  3.9336e-03,\n",
            "         -1.8589e-03,  3.6208e-03, -1.0204e-03,  8.2034e-03, -1.4346e-03],\n",
            "        [ 3.2191e-02,  7.0345e-02, -2.0485e-01,  1.0013e-02,  1.1179e-01,\n",
            "         -5.2827e-02,  1.0290e-01, -2.8999e-02,  2.3313e-01, -4.0770e-02]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000,  0.2864,  0.0572, -0.0056, -0.1604])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.0000,  0.1248,  0.0823,  0.0182,  0.0675],\n",
            "        [-0.0000, -0.3512, -0.2317, -0.0513, -0.1899],\n",
            "        [ 0.0000,  0.2264,  0.1494,  0.0331,  0.1224]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.2166, -0.6095,  0.3929])\n",
            "\n",
            "Model parameters updated after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[ 0.1480,  0.2802, -0.2335,  0.2980, -0.1633, -0.3148,  0.2585,  0.0934,\n",
            "          0.2281, -0.1704],\n",
            "        [-0.1352, -0.2604,  0.0678, -0.2899, -0.1301,  0.1817,  0.0465,  0.2301,\n",
            "          0.0714, -0.0970],\n",
            "        [ 0.1179,  0.2323,  0.1881,  0.2115, -0.3028,  0.2866,  0.2143,  0.0812,\n",
            "         -0.2584,  0.0404],\n",
            "        [ 0.0360,  0.2943,  0.3013, -0.0554,  0.0199,  0.1684,  0.1030, -0.2683,\n",
            "          0.2331, -0.1029],\n",
            "        [-0.2985,  0.2256, -0.2875, -0.1112, -0.0675,  0.1515,  0.1505,  0.1754,\n",
            "         -0.2525,  0.2424]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.2124,  0.2897, -0.2965,  0.2768,  0.2565], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[ 0.1267, -0.4413, -0.1252,  0.1516, -0.1737],\n",
            "        [ 0.1573, -0.4398, -0.2237,  0.0976,  0.2467],\n",
            "        [-0.0812,  0.2868, -0.1356,  0.0503,  0.0672]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([-0.4123,  0.0922, -0.2986], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Question 8: **How do you train a neural network in PyTorch?**\n",
        "\n",
        "##### Answer:\n",
        "Training a neural network in PyTorch involves several steps:\n",
        "1. Forward pass: Compute the output.\n",
        "2. Compute loss: Use a loss function to compare predictions and targets.\n",
        "3. Backward pass: Compute gradients of the loss with respect to the weights.\n",
        "4. Update weights: Use an optimizer to update the weights based on the gradients.\n",
        "\n",
        "##### Example:\n"
      ],
      "metadata": {
        "id": "bjLNh793PLEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define a simple neural network with one hidden layer and one output layer\n",
        "class SimpleNeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNeuralNet, self).__init__()\n",
        "\n",
        "        # Hidden layer (fully connected)\n",
        "        self.hidden = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Output layer (fully connected)\n",
        "        self.output = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    # Define the forward pass through the network\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the hidden layer\n",
        "        print(f\"Input to hidden layer:\\n{x}\")\n",
        "        x = self.hidden(x)\n",
        "        print(f\"Output of hidden layer (before activation):\\n{x}\")\n",
        "\n",
        "        # Apply ReLU activation to the hidden layer output\n",
        "        x = torch.relu(x)\n",
        "        print(f\"Output of hidden layer (after ReLU activation):\\n{x}\")\n",
        "\n",
        "        # Pass through the output layer (logits, no activation here)\n",
        "        x = self.output(x)\n",
        "        print(f\"Output of final layer (logits, before softmax):\\n{x}\")\n",
        "\n",
        "        return x\n",
        "\n",
        "# Instantiate the model with:\n",
        "# - 10 input features\n",
        "# - 5 neurons in the hidden layer\n",
        "# - 3 output neurons (corresponding to 3 classes for classification)\n",
        "model = SimpleNeuralNet(input_size=10, hidden_size=5, output_size=3)\n",
        "print(\"\\nInitialized model:\\n\", model)\n",
        "\n",
        "# Define the Adam optimizer and CrossEntropyLoss (for classification)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop over 10 epochs\n",
        "for epoch in range(10):\n",
        "    print(f\"\\nEpoch {epoch + 1}/100\")  # Print current epoch\n",
        "\n",
        "    # Generate a random input tensor (1 sample, 10 features)\n",
        "    input_data = torch.randn(1, 10)\n",
        "    print(f\"\\nRandom input data (1 sample, 10 features):\\n{input_data}\")\n",
        "\n",
        "    # Define the target as class 1 (ground truth)\n",
        "    target = torch.tensor([1])\n",
        "    print(f\"\\nTarget class index (ground truth):\\n{target}\")\n",
        "\n",
        "    # Forward pass: Get model's output (logits) for this random input\n",
        "    output = model(input_data)\n",
        "\n",
        "    # Compute the loss between the model's output and the true target\n",
        "    loss = criterion(output, target)\n",
        "    print(f\"\\nLoss value (Cross-Entropy Loss): {loss.item()}\")\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Clear gradients from previous epoch\n",
        "    print(\"\\nGradients cleared.\")\n",
        "\n",
        "    loss.backward()  # Backpropagate the loss to compute gradients\n",
        "    print(\"Gradients after backward pass:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            print(f\"{name}.grad:\\n{param.grad}\")\n",
        "\n",
        "    optimizer.step()  # Update the model weights based on gradients\n",
        "    print(\"\\nUpdated model parameters after optimizer step:\")\n",
        "    for name, param in model.named_parameters():\n",
        "        print(f\"{name}:\\n{param}\")\n",
        "\n",
        "    # Print loss every 10 epochs for monitoring\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"\\n[Epoch {epoch}] Loss: {loss.item()}\\n\" + \"=\"*50)\n",
        "\n",
        "    print(f\"END OF EPOCH No: {epoch}\\n\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN8dOf8LPMk0",
        "outputId": "4aa88f59-e431-49aa-95e6-32e12687d42a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initialized model:\n",
            " SimpleNeuralNet(\n",
            "  (hidden): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (output): Linear(in_features=5, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "Epoch 1/100\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[-0.8018, -2.1324, -0.9448, -1.1086, -0.4982,  1.6505,  0.2798,  0.1208,\n",
            "          0.6454,  0.7081]])\n",
            "\n",
            "Target class index (ground truth):\n",
            "tensor([1])\n",
            "Input to hidden layer:\n",
            "tensor([[-0.8018, -2.1324, -0.9448, -1.1086, -0.4982,  1.6505,  0.2798,  0.1208,\n",
            "          0.6454,  0.7081]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[-0.2597,  0.4406,  0.3717,  0.4491, -0.1756]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.0000, 0.4406, 0.3717, 0.4491, 0.0000]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[-0.1617,  0.0173, -0.1608]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss value (Cross-Entropy Loss): 0.983154296875\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[-0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-0.1109, -0.2949, -0.1307, -0.1533, -0.0689,  0.2283,  0.0387,  0.0167,\n",
            "          0.0893,  0.0979],\n",
            "        [ 0.0724,  0.1926,  0.0854,  0.1002,  0.0450, -0.1491, -0.0253, -0.0109,\n",
            "         -0.0583, -0.0640],\n",
            "        [ 0.1187,  0.3157,  0.1399,  0.1641,  0.0738, -0.2444, -0.0414, -0.0179,\n",
            "         -0.0956, -0.1048],\n",
            "        [-0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000,  0.1383, -0.0903, -0.1481,  0.0000])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.0000,  0.1378,  0.1163,  0.1405,  0.0000],\n",
            "        [-0.0000, -0.2757, -0.2327, -0.2811, -0.0000],\n",
            "        [ 0.0000,  0.1379,  0.1164,  0.1406,  0.0000]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.3128, -0.6259,  0.3131])\n",
            "\n",
            "Updated model parameters after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2457, -0.0575,  0.0466,  0.0976,  0.2164, -0.1405, -0.1501,  0.1238,\n",
            "         -0.2967,  0.1614],\n",
            "        [ 0.0169, -0.1621, -0.2772, -0.0131, -0.0887, -0.1108, -0.1203, -0.0212,\n",
            "         -0.1081, -0.1268],\n",
            "        [ 0.2413, -0.0990, -0.2466, -0.2236, -0.2512,  0.0745, -0.1870,  0.1751,\n",
            "         -0.2787, -0.1198],\n",
            "        [-0.1368, -0.1033, -0.0511, -0.2037, -0.1391, -0.0186,  0.1583, -0.2404,\n",
            "          0.0320, -0.1415],\n",
            "        [-0.1565,  0.2895, -0.2694, -0.1174,  0.2649, -0.0059,  0.2162,  0.2363,\n",
            "          0.1558,  0.0683]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0169,  0.1567, -0.0692, -0.1195, -0.1645], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.0710, -0.3550, -0.3787, -0.2645, -0.0956],\n",
            "        [ 0.1946, -0.4013,  0.0016,  0.0628, -0.4472],\n",
            "        [-0.4073, -0.0099,  0.0890, -0.0871, -0.0850]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.2519,  0.1676, -0.1527], requires_grad=True)\n",
            "\n",
            "[Epoch 0] Loss: 0.983154296875\n",
            "==================================================\n",
            "END OF EPOCH No: 0\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 2/100\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[ 1.0114, -0.7331,  0.5108, -0.5076,  1.7327, -0.4530,  1.5997, -0.2563,\n",
            "         -1.7923,  0.3439]])\n",
            "\n",
            "Target class index (ground truth):\n",
            "tensor([1])\n",
            "Input to hidden layer:\n",
            "tensor([[ 1.0114, -0.7331,  0.5108, -0.5076,  1.7327, -0.4530,  1.5997, -0.2563,\n",
            "         -1.7923,  0.3439]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[ 0.5388,  0.0175, -0.1197, -0.1286, -0.1218]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.5388, 0.0175, 0.0000, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[ 0.2075,  0.2654, -0.3723]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss value (Cross-Entropy Loss): 0.9051255583763123\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[-0.2327,  0.1686, -0.1175,  0.1168, -0.3986,  0.1042, -0.3680,  0.0590,\n",
            "          0.4123, -0.0791],\n",
            "        [ 0.1025, -0.0743,  0.0518, -0.0514,  0.1756, -0.0459,  0.1622, -0.0260,\n",
            "         -0.1817,  0.0349],\n",
            "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "         -0.0000,  0.0000],\n",
            "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "         -0.0000,  0.0000],\n",
            "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "         -0.0000,  0.0000]])\n",
            "hidden.bias.grad:\n",
            "tensor([-0.2300,  0.1014,  0.0000,  0.0000,  0.0000])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.2057,  0.0067,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.3208, -0.0104, -0.0000, -0.0000, -0.0000],\n",
            "        [ 0.1152,  0.0037,  0.0000,  0.0000,  0.0000]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.3817, -0.5955,  0.2138])\n",
            "\n",
            "Updated model parameters after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2449, -0.0582,  0.0473,  0.0969,  0.2172, -0.1413, -0.1494,  0.1231,\n",
            "         -0.2974,  0.1622],\n",
            "        [ 0.0169, -0.1613, -0.2769, -0.0122, -0.0891, -0.1113, -0.1211, -0.0209,\n",
            "         -0.1077, -0.1277],\n",
            "        [ 0.2406, -0.0997, -0.2473, -0.2242, -0.2519,  0.0752, -0.1863,  0.1757,\n",
            "         -0.2781, -0.1191],\n",
            "        [-0.1375, -0.1040, -0.0518, -0.2044, -0.1398, -0.0179,  0.1590, -0.2398,\n",
            "          0.0327, -0.1408],\n",
            "        [-0.1565,  0.2895, -0.2694, -0.1174,  0.2649, -0.0059,  0.2162,  0.2363,\n",
            "          0.1558,  0.0683]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0176,  0.1557, -0.0685, -0.1188, -0.1645], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.0718, -0.3557, -0.3793, -0.2651, -0.0956],\n",
            "        [ 0.1953, -0.4006,  0.0023,  0.0635, -0.4472],\n",
            "        [-0.4080, -0.0106,  0.0884, -0.0878, -0.0850]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.2509,  0.1686, -0.1537], requires_grad=True)\n",
            "END OF EPOCH No: 1\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 3/100\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[ 0.3231,  1.3379, -1.6253,  0.1136, -0.6652,  2.7664,  1.0699, -0.5888,\n",
            "         -0.0862,  0.3037]])\n",
            "\n",
            "Target class index (ground truth):\n",
            "tensor([1])\n",
            "Input to hidden layer:\n",
            "tensor([[ 0.3231,  1.3379, -1.6253,  0.1136, -0.6652,  2.7664,  1.0699, -0.5888,\n",
            "         -0.0862,  0.3037]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[-0.8980, -0.0015,  0.3128,  0.0677,  0.5038]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.0000, 0.0000, 0.3128, 0.0677, 0.5038]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[ 0.0662, -0.0517, -0.1748]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss value (Cross-Entropy Loss): 1.1016758680343628\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
            "         -0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
            "         -0.0000,  0.0000],\n",
            "        [-0.0379, -0.1571,  0.1908, -0.0133,  0.0781, -0.3248, -0.1256,  0.0691,\n",
            "          0.0101, -0.0357],\n",
            "        [-0.0540, -0.2238,  0.2719, -0.0190,  0.1113, -0.4628, -0.1790,  0.0985,\n",
            "          0.0144, -0.0508],\n",
            "        [ 0.0768,  0.3182, -0.3866,  0.0270, -0.1582,  0.6580,  0.2545, -0.1400,\n",
            "         -0.0205,  0.0722]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000,  0.0000, -0.1174, -0.1673,  0.2378])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.0000,  0.0000,  0.1169,  0.0253,  0.1883],\n",
            "        [-0.0000, -0.0000, -0.2088, -0.0452, -0.3364],\n",
            "        [ 0.0000,  0.0000,  0.0919,  0.0199,  0.1480]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.3739, -0.6677,  0.2938])\n",
            "\n",
            "Updated model parameters after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2444, -0.0588,  0.0479,  0.0963,  0.2177, -0.1418, -0.1488,  0.1225,\n",
            "         -0.2980,  0.1627],\n",
            "        [ 0.0169, -0.1606, -0.2766, -0.0115, -0.0895, -0.1117, -0.1218, -0.0207,\n",
            "         -0.1075, -0.1284],\n",
            "        [ 0.2404, -0.0997, -0.2481, -0.2247, -0.2527,  0.0760, -0.1856,  0.1752,\n",
            "         -0.2777, -0.1183],\n",
            "        [-0.1377, -0.1040, -0.0526, -0.2049, -0.1406, -0.0171,  0.1597, -0.2403,\n",
            "          0.0331, -0.1401],\n",
            "        [-0.1571,  0.2889, -0.2687, -0.1180,  0.2656, -0.0065,  0.2155,  0.2369,\n",
            "          0.1564,  0.0676]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0182,  0.1550, -0.0677, -0.1180, -0.1651], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.0723, -0.3563, -0.3801, -0.2657, -0.0963],\n",
            "        [ 0.1959, -0.4001,  0.0031,  0.0641, -0.4465],\n",
            "        [-0.4086, -0.0111,  0.0876, -0.0884, -0.0856]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.2499,  0.1696, -0.1547], requires_grad=True)\n",
            "END OF EPOCH No: 2\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 4/100\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[ 0.4919, -2.0803, -0.5156, -0.3976, -0.4430,  0.1531, -0.0702,  1.2369,\n",
            "         -0.6403,  0.1594]])\n",
            "\n",
            "Target class index (ground truth):\n",
            "tensor([1])\n",
            "Input to hidden layer:\n",
            "tensor([[ 0.4919, -2.0803, -0.5156, -0.3976, -0.4430,  0.1531, -0.0702,  1.2369,\n",
            "         -0.6403,  0.1594]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[ 0.2179,  0.6984,  0.9874, -0.1531, -0.5880]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.2179, 0.6984, 0.9874, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[-0.3900, -0.0641, -0.1650]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss value (Cross-Entropy Loss): 0.9654148817062378\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[-0.1386,  0.5863,  0.1453,  0.1120,  0.1248, -0.0432,  0.0198, -0.3486,\n",
            "          0.1805, -0.0449],\n",
            "        [ 0.0718, -0.3037, -0.0753, -0.0580, -0.0647,  0.0224, -0.0102,  0.1805,\n",
            "         -0.0935,  0.0233],\n",
            "        [-0.0375,  0.1587,  0.0393,  0.0303,  0.0338, -0.0117,  0.0054, -0.0944,\n",
            "          0.0488, -0.0122],\n",
            "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
            "         -0.0000,  0.0000],\n",
            "        [ 0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
            "         -0.0000,  0.0000]])\n",
            "hidden.bias.grad:\n",
            "tensor([-0.2818,  0.1460, -0.0763,  0.0000,  0.0000])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.0599,  0.1920,  0.2714,  0.0000,  0.0000],\n",
            "        [-0.1349, -0.4325, -0.6114, -0.0000, -0.0000],\n",
            "        [ 0.0750,  0.2405,  0.3399,  0.0000,  0.0000]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.2749, -0.6192,  0.3443])\n",
            "\n",
            "Updated model parameters after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2437, -0.0595,  0.0477,  0.0956,  0.2180, -0.1421, -0.1484,  0.1230,\n",
            "         -0.2987,  0.1634],\n",
            "        [ 0.0167, -0.1599, -0.2761, -0.0108, -0.0896, -0.1121, -0.1223, -0.0213,\n",
            "         -0.1070, -0.1290],\n",
            "        [ 0.2406, -0.1000, -0.2488, -0.2252, -0.2535,  0.0767, -0.1850,  0.1754,\n",
            "         -0.2778, -0.1176],\n",
            "        [-0.1379, -0.1041, -0.0532, -0.2052, -0.1413, -0.0164,  0.1603, -0.2408,\n",
            "          0.0334, -0.1395],\n",
            "        [-0.1576,  0.2884, -0.2682, -0.1185,  0.2661, -0.0070,  0.2150,  0.2375,\n",
            "          0.1570,  0.0671]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0189,  0.1541, -0.0668, -0.1173, -0.1656], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.0729, -0.3570, -0.3810, -0.2663, -0.0968],\n",
            "        [ 0.1966, -0.3994,  0.0039,  0.0646, -0.4460],\n",
            "        [-0.4093, -0.0118,  0.0868, -0.0889, -0.0861]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.2490,  0.1706, -0.1556], requires_grad=True)\n",
            "END OF EPOCH No: 3\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 5/100\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[-0.8806,  1.3909, -1.3871, -0.6580, -1.5801,  0.9586, -0.0940,  1.2621,\n",
            "         -0.5229, -2.0929]])\n",
            "\n",
            "Target class index (ground truth):\n",
            "tensor([1])\n",
            "Input to hidden layer:\n",
            "tensor([[-0.8806,  1.3909, -1.3871, -0.6580, -1.5801,  0.9586, -0.0940,  1.2621,\n",
            "         -0.5229, -2.0929]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[-0.4757,  0.6519,  1.1797,  0.2312,  0.4541]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.0000, 0.6519, 1.1797, 0.2312, 0.4541]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[-0.5388, -0.2727, -0.1207]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss value (Cross-Entropy Loss): 1.0751938819885254\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[-0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
            "         -0.0000, -0.0000],\n",
            "        [-0.1453,  0.2295, -0.2289, -0.1086, -0.2608,  0.1582, -0.0155,  0.2083,\n",
            "         -0.0863, -0.3454],\n",
            "        [ 0.0597, -0.0943,  0.0940,  0.0446,  0.1071, -0.0650,  0.0064, -0.0855,\n",
            "          0.0354,  0.1418],\n",
            "        [ 0.1299, -0.2051,  0.2046,  0.0970,  0.2331, -0.1414,  0.0139, -0.1861,\n",
            "          0.0771,  0.3087],\n",
            "        [-0.2063,  0.3259, -0.3250, -0.1542, -0.3702,  0.2246, -0.0220,  0.2957,\n",
            "         -0.1225, -0.4903]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000,  0.1650, -0.0678, -0.1475,  0.2343])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.0000,  0.1705,  0.3085,  0.0605,  0.1187],\n",
            "        [-0.0000, -0.4295, -0.7772, -0.1523, -0.2991],\n",
            "        [ 0.0000,  0.2590,  0.4687,  0.0919,  0.1804]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.2615, -0.6588,  0.3973])\n",
            "\n",
            "Updated model parameters after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2431, -0.0601,  0.0476,  0.0949,  0.2182, -0.1422, -0.1480,  0.1234,\n",
            "         -0.2992,  0.1640],\n",
            "        [ 0.0169, -0.1595, -0.2754, -0.0100, -0.0892, -0.1127, -0.1227, -0.0220,\n",
            "         -0.1064, -0.1287],\n",
            "        [ 0.2403, -0.1001, -0.2496, -0.2258, -0.2544,  0.0773, -0.1846,  0.1758,\n",
            "         -0.2781, -0.1178],\n",
            "        [-0.1384, -0.1038, -0.0540, -0.2058, -0.1420, -0.0158,  0.1608, -0.2404,\n",
            "          0.0333, -0.1398],\n",
            "        [-0.1573,  0.2877, -0.2675, -0.1181,  0.2668, -0.0076,  0.2146,  0.2372,\n",
            "          0.1576,  0.0676]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0196,  0.1532, -0.0659, -0.1166, -0.1663], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.0735, -0.3578, -0.3819, -0.2669, -0.0975],\n",
            "        [ 0.1971, -0.3986,  0.0048,  0.0652, -0.4453],\n",
            "        [-0.4099, -0.0127,  0.0859, -0.0895, -0.0868]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.2480,  0.1716, -0.1567], requires_grad=True)\n",
            "END OF EPOCH No: 4\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 6/100\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[-0.3865,  0.5962, -0.4742, -0.2405,  0.5190, -0.4490,  0.7643, -0.4506,\n",
            "          0.8327,  0.5459]])\n",
            "\n",
            "Target class index (ground truth):\n",
            "tensor([1])\n",
            "Input to hidden layer:\n",
            "tensor([[-0.3865,  0.5962, -0.4742, -0.2405,  0.5190, -0.4490,  0.7643, -0.4506,\n",
            "          0.8327,  0.5459]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[-0.1189, -0.0538, -0.7288,  0.0661,  0.5883]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.0661, 0.5883]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[ 0.1730, -0.0861, -0.2137]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss value (Cross-Entropy Loss): 1.1556172370910645\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[-0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [-0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "          0.0000,  0.0000],\n",
            "        [ 0.0689, -0.1064,  0.0846,  0.0429, -0.0926,  0.0801, -0.1363,  0.0804,\n",
            "         -0.1485, -0.0974],\n",
            "        [-0.0932,  0.1439, -0.1144, -0.0580,  0.1252, -0.1083,  0.1844, -0.1087,\n",
            "          0.2009,  0.1317]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000,  0.0000,  0.0000, -0.1784,  0.2413])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  0.0270,  0.2400],\n",
            "        [-0.0000, -0.0000, -0.0000, -0.0453, -0.4031],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.0183,  0.1631]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.4080, -0.6851,  0.2772])\n",
            "\n",
            "Updated model parameters after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2426, -0.0606,  0.0475,  0.0944,  0.2184, -0.1424, -0.1477,  0.1238,\n",
            "         -0.2997,  0.1645],\n",
            "        [ 0.0170, -0.1592, -0.2749, -0.0093, -0.0888, -0.1132, -0.1231, -0.0226,\n",
            "         -0.1060, -0.1284],\n",
            "        [ 0.2402, -0.1002, -0.2503, -0.2263, -0.2552,  0.0779, -0.1842,  0.1762,\n",
            "         -0.2784, -0.1180],\n",
            "        [-0.1389, -0.1035, -0.0548, -0.2063, -0.1425, -0.0153,  0.1615, -0.2404,\n",
            "          0.0337, -0.1399],\n",
            "        [-0.1568,  0.2869, -0.2668, -0.1175,  0.2672, -0.0081,  0.2140,  0.2371,\n",
            "          0.1574,  0.0678]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0201,  0.1524, -0.0652, -0.1157, -0.1671], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.0739, -0.3585, -0.3826, -0.2675, -0.0982],\n",
            "        [ 0.1976, -0.3978,  0.0056,  0.0658, -0.4445],\n",
            "        [-0.4104, -0.0134,  0.0852, -0.0901, -0.0876]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.2470,  0.1726, -0.1576], requires_grad=True)\n",
            "END OF EPOCH No: 5\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 7/100\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[-1.6383,  0.6885,  0.6986, -0.4441, -1.7784, -0.4159, -0.5343,  1.6680,\n",
            "         -0.3484, -0.0248]])\n",
            "\n",
            "Target class index (ground truth):\n",
            "tensor([1])\n",
            "Input to hidden layer:\n",
            "tensor([[-1.6383,  0.6885,  0.6986, -0.4441, -1.7784, -0.4159, -0.5343,  1.6680,\n",
            "         -0.3484, -0.0248]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[ 0.4235,  0.1002,  0.3117, -0.1417, -0.0941]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.4235, 0.1002, 0.3117, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[ 0.0605,  0.2181, -0.3062]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss value (Cross-Entropy Loss): 0.8944967985153198\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.3964, -0.1666, -0.1690,  0.1075,  0.4303,  0.1006,  0.1293, -0.4036,\n",
            "          0.0843,  0.0060],\n",
            "        [-0.1749,  0.0735,  0.0746, -0.0474, -0.1899, -0.0444, -0.0570,  0.1781,\n",
            "         -0.0372, -0.0026],\n",
            "        [ 0.1905, -0.0801, -0.0812,  0.0516,  0.2068,  0.0484,  0.0621, -0.1940,\n",
            "          0.0405,  0.0029],\n",
            "        [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "         -0.0000, -0.0000],\n",
            "        [-0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
            "         -0.0000, -0.0000]])\n",
            "hidden.bias.grad:\n",
            "tensor([-0.2419,  0.1068, -0.1163,  0.0000,  0.0000])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.1479,  0.0350,  0.1088,  0.0000,  0.0000],\n",
            "        [-0.2503, -0.0593, -0.1843, -0.0000, -0.0000],\n",
            "        [ 0.1025,  0.0243,  0.0754,  0.0000,  0.0000]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.3492, -0.5912,  0.2420])\n",
            "\n",
            "Updated model parameters after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2427, -0.0609,  0.0478,  0.0937,  0.2182, -0.1428, -0.1476,  0.1243,\n",
            "         -0.3002,  0.1649],\n",
            "        [ 0.0174, -0.1591, -0.2745, -0.0086, -0.0883, -0.1135, -0.1232, -0.0233,\n",
            "         -0.1055, -0.1281],\n",
            "        [ 0.2396, -0.1001, -0.2507, -0.2269, -0.2559,  0.0783, -0.1841,  0.1768,\n",
            "         -0.2788, -0.1182],\n",
            "        [-0.1395, -0.1033, -0.0554, -0.2068, -0.1429, -0.0148,  0.1620, -0.2403,\n",
            "          0.0340, -0.1401],\n",
            "        [-0.1564,  0.2863, -0.2662, -0.1170,  0.2675, -0.0084,  0.2135,  0.2370,\n",
            "          0.1572,  0.0681]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0208,  0.1516, -0.0644, -0.1150, -0.1678], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.0745, -0.3592, -0.3834, -0.2680, -0.0989],\n",
            "        [ 0.1982, -0.3972,  0.0063,  0.0664, -0.4438],\n",
            "        [-0.4111, -0.0140,  0.0845, -0.0907, -0.0883]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.2460,  0.1736, -0.1586], requires_grad=True)\n",
            "END OF EPOCH No: 6\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 8/100\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[-0.0902,  0.1458, -0.2548, -1.5572, -0.9273, -0.7370, -0.9042, -0.8078,\n",
            "         -0.7364,  0.3461]])\n",
            "\n",
            "Target class index (ground truth):\n",
            "tensor([1])\n",
            "Input to hidden layer:\n",
            "tensor([[-0.0902,  0.1458, -0.2548, -1.5572, -0.9273, -0.7370, -0.9042, -0.8078,\n",
            "         -0.7364,  0.3461]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[ 0.0898,  0.5393,  0.6843,  0.3363, -0.5805]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.0898, 0.5393, 0.6843, 0.3363, 0.0000]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[-0.3069,  0.0038, -0.1758]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss value (Cross-Entropy Loss): 0.9433268308639526\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[ 0.0249, -0.0402,  0.0703,  0.4299,  0.2560,  0.2035,  0.2496,  0.2230,\n",
            "          0.2033, -0.0955],\n",
            "        [-0.0122,  0.0198, -0.0345, -0.2110, -0.1256, -0.0999, -0.1225, -0.1095,\n",
            "         -0.0998,  0.0469],\n",
            "        [ 0.0077, -0.0125,  0.0218,  0.1335,  0.0795,  0.0632,  0.0775,  0.0693,\n",
            "          0.0631, -0.0297],\n",
            "        [ 0.0132, -0.0214,  0.0373,  0.2281,  0.1358,  0.1080,  0.1325,  0.1183,\n",
            "          0.1079, -0.0507],\n",
            "        [-0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
            "         -0.0000,  0.0000]])\n",
            "hidden.bias.grad:\n",
            "tensor([-0.2761,  0.1355, -0.0858, -0.1465,  0.0000])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.0256,  0.1539,  0.1953,  0.0960,  0.0000],\n",
            "        [-0.0548, -0.3293, -0.4179, -0.2054, -0.0000],\n",
            "        [ 0.0292,  0.1755,  0.2226,  0.1094,  0.0000]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.2853, -0.6107,  0.3253])\n",
            "\n",
            "Updated model parameters after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2429, -0.0611,  0.0478,  0.0930,  0.2178, -0.1435, -0.1478,  0.1246,\n",
            "         -0.3008,  0.1656],\n",
            "        [ 0.0178, -0.1590, -0.2741, -0.0078, -0.0877, -0.1136, -0.1230, -0.0238,\n",
            "         -0.1049, -0.1279],\n",
            "        [ 0.2392, -0.1000, -0.2511, -0.2277, -0.2568,  0.0786, -0.1843,  0.1772,\n",
            "         -0.2794, -0.1182],\n",
            "        [-0.1399, -0.1030, -0.0561, -0.2075, -0.1434, -0.0145,  0.1622, -0.2405,\n",
            "          0.0339, -0.1401],\n",
            "        [-0.1560,  0.2857, -0.2657, -0.1166,  0.2678, -0.0088,  0.2131,  0.2369,\n",
            "          0.1571,  0.0683]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0216,  0.1508, -0.0635, -0.1142, -0.1684], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.0751, -0.3600, -0.3842, -0.2687, -0.0995],\n",
            "        [ 0.1988, -0.3964,  0.0071,  0.0670, -0.4432],\n",
            "        [-0.4117, -0.0147,  0.0837, -0.0913, -0.0889]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.2450,  0.1746, -0.1596], requires_grad=True)\n",
            "END OF EPOCH No: 7\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 9/100\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[-0.7674,  0.5656, -2.0186, -1.3354,  0.0860, -0.3783, -0.8024, -1.6934,\n",
            "          0.7568, -2.0130]])\n",
            "\n",
            "Target class index (ground truth):\n",
            "tensor([1])\n",
            "Input to hidden layer:\n",
            "tensor([[-0.7674,  0.5656, -2.0186, -1.3354,  0.0860, -0.3783, -0.8024, -1.6934,\n",
            "          0.7568, -2.0130]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[-0.6278,  0.9635,  0.3298,  0.9031,  0.2405]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.0000, 0.9635, 0.3298, 0.9031, 0.2405]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[-0.4951, -0.2511, -0.2501]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss value (Cross-Entropy Loss): 1.0240888595581055\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[-0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
            "          0.0000, -0.0000],\n",
            "        [-0.1132,  0.0834, -0.2977, -0.1969,  0.0127, -0.0558, -0.1183, -0.2497,\n",
            "          0.1116, -0.2969],\n",
            "        [ 0.0633, -0.0467,  0.1666,  0.1102, -0.0071,  0.0312,  0.0662,  0.1398,\n",
            "         -0.0625,  0.1662],\n",
            "        [ 0.1162, -0.0856,  0.3056,  0.2022, -0.0130,  0.0573,  0.1215,  0.2564,\n",
            "         -0.1146,  0.3047],\n",
            "        [-0.1719,  0.1267, -0.4523, -0.2992,  0.0193, -0.0848, -0.1798, -0.3794,\n",
            "          0.1696, -0.4510]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000,  0.1475, -0.0825, -0.1514,  0.2241])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.0000,  0.2711,  0.0928,  0.2541,  0.0677],\n",
            "        [-0.0000, -0.6175, -0.2113, -0.5788, -0.1542],\n",
            "        [ 0.0000,  0.3464,  0.1185,  0.3247,  0.0865]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.2814, -0.6409,  0.3595])\n",
            "\n",
            "Updated model parameters after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2430, -0.0613,  0.0479,  0.0924,  0.2175, -0.1440, -0.1480,  0.1249,\n",
            "         -0.3014,  0.1661],\n",
            "        [ 0.0184, -0.1589, -0.2735, -0.0070, -0.0872, -0.1137, -0.1226, -0.0238,\n",
            "         -0.1046, -0.1275],\n",
            "        [ 0.2386, -0.0999, -0.2517, -0.2285, -0.2575,  0.0788, -0.1846,  0.1773,\n",
            "         -0.2796, -0.1186],\n",
            "        [-0.1405, -0.1027, -0.0568, -0.2082, -0.1438, -0.0143,  0.1621, -0.2410,\n",
            "          0.0341, -0.1404],\n",
            "        [-0.1554,  0.2851, -0.2650, -0.1160,  0.2681, -0.0090,  0.2130,  0.2373,\n",
            "          0.1567,  0.0687]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0223,  0.1499, -0.0626, -0.1133, -0.1691], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.0756, -0.3608, -0.3850, -0.2694, -0.1001],\n",
            "        [ 0.1994, -0.3956,  0.0078,  0.0678, -0.4426],\n",
            "        [-0.4123, -0.0156,  0.0830, -0.0920, -0.0896]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.2440,  0.1756, -0.1606], requires_grad=True)\n",
            "END OF EPOCH No: 8\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Epoch 10/100\n",
            "\n",
            "Random input data (1 sample, 10 features):\n",
            "tensor([[-0.0485,  0.0961,  1.0708, -1.0208,  0.1145, -0.3981,  1.8788, -2.4547,\n",
            "         -0.3817, -0.1894]])\n",
            "\n",
            "Target class index (ground truth):\n",
            "tensor([1])\n",
            "Input to hidden layer:\n",
            "tensor([[-0.0485,  0.0961,  1.0708, -1.0208,  0.1145, -0.3981,  1.8788, -2.4547,\n",
            "         -0.3817, -0.1894]])\n",
            "Output of hidden layer (before activation):\n",
            "tensor([[-0.4337, -0.2246, -0.8337,  0.9343, -0.5204]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "Output of hidden layer (after ReLU activation):\n",
            "tensor([[0.0000, 0.0000, 0.0000, 0.9343, 0.0000]], grad_fn=<ReluBackward0>)\n",
            "Output of final layer (logits, before softmax):\n",
            "tensor([[-0.0077,  0.2389, -0.2466]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Loss value (Cross-Entropy Loss): 0.8741565942764282\n",
            "\n",
            "Gradients cleared.\n",
            "Gradients after backward pass:\n",
            "hidden.weight.grad:\n",
            "tensor([[-0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "         -0.0000, -0.0000],\n",
            "        [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "         -0.0000, -0.0000],\n",
            "        [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "         -0.0000, -0.0000],\n",
            "        [ 0.0073, -0.0145, -0.1617,  0.1541, -0.0173,  0.0601, -0.2836,  0.3706,\n",
            "          0.0576,  0.0286],\n",
            "        [-0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
            "         -0.0000, -0.0000]])\n",
            "hidden.bias.grad:\n",
            "tensor([ 0.0000,  0.0000,  0.0000, -0.1510,  0.0000])\n",
            "output.weight.grad:\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  0.3046,  0.0000],\n",
            "        [-0.0000, -0.0000, -0.0000, -0.5445, -0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  0.2399,  0.0000]])\n",
            "output.bias.grad:\n",
            "tensor([ 0.3260, -0.5828,  0.2567])\n",
            "\n",
            "Updated model parameters after optimizer step:\n",
            "hidden.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.2432, -0.0615,  0.0480,  0.0919,  0.2172, -0.1445, -0.1481,  0.1251,\n",
            "         -0.3019,  0.1666],\n",
            "        [ 0.0188, -0.1589, -0.2730, -0.0062, -0.0868, -0.1137, -0.1222, -0.0239,\n",
            "         -0.1043, -0.1271],\n",
            "        [ 0.2381, -0.0998, -0.2522, -0.2292, -0.2581,  0.0790, -0.1849,  0.1773,\n",
            "         -0.2798, -0.1189],\n",
            "        [-0.1411, -0.1024, -0.0573, -0.2091, -0.1442, -0.0142,  0.1624, -0.2416,\n",
            "          0.0342, -0.1407],\n",
            "        [-0.1549,  0.2846, -0.2644, -0.1155,  0.2683, -0.0092,  0.2129,  0.2376,\n",
            "          0.1564,  0.0691]], requires_grad=True)\n",
            "hidden.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.0229,  0.1491, -0.0618, -0.1125, -0.1698], requires_grad=True)\n",
            "output.weight:\n",
            "Parameter containing:\n",
            "tensor([[-0.0761, -0.3615, -0.3858, -0.2702, -0.1007],\n",
            "        [ 0.1999, -0.3949,  0.0085,  0.0686, -0.4420],\n",
            "        [-0.4129, -0.0163,  0.0823, -0.0929, -0.0902]], requires_grad=True)\n",
            "output.bias:\n",
            "Parameter containing:\n",
            "tensor([ 0.2431,  0.1766, -0.1616], requires_grad=True)\n",
            "END OF EPOCH No: 9\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###### Explanation of Output:\n",
        "\n",
        "1. **Model Architecture**: Displays the layers (hidden and output) with input/output dimensions.\n",
        "2. **Random Input Data**: Shows the generated random input tensor with 10 features.\n",
        "3. **Forward Pass**:\n",
        "   - The input passes through the hidden layer, ReLU activation, and the output layer, with intermediate results printed.\n",
        "4. **Loss Calculation**: Shows the loss value (Cross-Entropy) after comparing the output logits with the target.\n",
        "5. **Backward Pass**:\n",
        "   - The gradients of the weights and biases are computed and displayed after backpropagation.\n",
        "6. **Weight Updates**: The updated weights and biases are printed after the Adam optimizer updates the model.\n",
        "\n",
        "By adding these print statements, the code provides a clear view of what happens during the forward pass, how the loss is calculated, how the gradients are computed during backpropagation, and how the model parameters are updated with the Adam optimizer. This helps make the entire training loop more transparent and easier to follow."
      ],
      "metadata": {
        "id": "paOVhUiBalMx"
      }
    }
  ]
}