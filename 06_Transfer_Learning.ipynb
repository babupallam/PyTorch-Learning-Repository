{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0GvSvIKmqY7rJlqoWvqIP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/PyTorch-Learning-Repository/blob/main/06_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "_VUeoGY2b3Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **6.1. What is Transfer Learning?**\n",
        "- **Transfer Learning** is the process of taking a pre-trained model on one task and adapting it to a new, often smaller, but related task.\n",
        "- This is particularly useful in deep learning, where training models from scratch on large datasets requires enormous computational resources and time.\n",
        "- In computer vision, common pre-trained models include **ResNet**, **VGG**, and **Inception**, which are often trained on **ImageNet** (a dataset of 1.2 million images across 1,000 classes).\n",
        "\n",
        "---\n",
        "\n",
        "**6.1.1. Key Concepts in Transfer Learning**\n",
        "- **Feature Extraction**: We freeze the convolutional layers of a pre-trained network and only train the fully connected layers on the new task. This approach uses the pre-trained model’s ability to extract features from images.\n",
        "- **Fine-tuning**: In this approach, we unfreeze some or all of the convolutional layers of the pre-trained model and retrain them along with the fully connected layers to adapt the model to the new dataset.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "42E2pL3y8NyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **6.2. Pre-trained Models in PyTorch**\n",
        "- PyTorch provides pre-trained models through `torchvision.models`, allowing easy access to models like ResNet, VGG, and DenseNet, among others.\n",
        "- We can load a pre-trained model by setting `pretrained=True` when initializing the model.\n",
        "\n",
        "**Common Pre-trained Models**:\n",
        "- **ResNet**: A very deep network with residual connections to alleviate vanishing gradient problems.\n",
        "- **VGG**: A simpler architecture that uses large convolutional filters but lacks residual connections.\n",
        "- **Inception**: A complex architecture with multiple types of convolutional operations in parallel.\n",
        "  \n",
        "---\n",
        "\n",
        "**6.2.1. Using a Pre-trained Model**\n",
        "We will now explore how to use a pre-trained model (ResNet18) and apply it to a new classification task using transfer learning. We will demonstrate both **feature extraction** and **fine-tuning** approaches.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1No9b3ac8O0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **6.3. Feature Extraction with a Pre-trained Model**\n",
        "- In this approach, we freeze the convolutional base of the pre-trained model and replace the final fully connected layer to match the number of classes in our new dataset.\n",
        "\n",
        "---\n",
        "\n",
        "**6.3.1. Example: Transfer Learning with ResNet18 (Feature Extraction)**\n",
        "\n",
        "**Demonstration: Feature Extraction using ResNet18**"
      ],
      "metadata": {
        "id": "EpSpCkJJ8O42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn  # Import neural network functionalities (layers, loss functions, etc.)\n",
        "import torch.optim as optim  # Import optimizers like Adam, SGD, etc.\n",
        "import torchvision  # Provides datasets, pre-trained models, and transforms\n",
        "import torchvision.transforms as transforms  # Tools for data preprocessing and augmentation\n",
        "from torchvision import models  # Module to load pre-trained models\n"
      ],
      "metadata": {
        "id": "ljWetlMh8O42"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: Define transformations for the dataset\n",
        "# Resize the CIFAR-10 images to 224x224 (as ResNet expects this input size) and normalize them\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # Resize images to 224x224 (ResNet input size)\n",
        "    transforms.ToTensor(),  # Convert images to PyTorch tensors (required for model input)\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image pixels to range [-1, 1]\n",
        "])\n"
      ],
      "metadata": {
        "id": "LQMUR2GzmjfT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 2: Load CIFAR-10 dataset (small dataset with 60,000 32x32 color images across 10 classes)\n",
        "# CIFAR-10 is being used as an example for loading a small custom dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROEk4Gctmjbx",
        "outputId": "f630a089-ad55-4c37-e895-86982c2609e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 3: Load the dataset into DataLoader for batching and shuffling\n",
        "# DataLoader helps in loading data in batches, making it easier to handle during training and testing\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)  # Shuffle training data\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)  # No need to shuffle test data\n"
      ],
      "metadata": {
        "id": "W2htKKiJmjZP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 4: Load a pre-trained ResNet-18 model\n",
        "# ResNet-18 is a deep neural network pre-trained on ImageNet. Here, we use it for transfer learning.\n",
        "model = models.resnet18(pretrained=True)  # Load the pre-trained ResNet-18 model\n"
      ],
      "metadata": {
        "id": "luWgYvWxmjMw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 5: Freeze the convolutional base of ResNet-18\n",
        "# Freezing means the weights of these layers will not be updated during training, allowing us to only train the new fully connected layer\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # Freeze all the layers (prevents weight updates)\n"
      ],
      "metadata": {
        "id": "5Zu9dMeJmoPC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 6: Replace the fully connected (fc) layer\n",
        "# ResNet's default final layer is designed for 1000 classes (ImageNet). We need to replace it with a new fully connected layer for CIFAR-10 (10 classes).\n",
        "num_ftrs = model.fc.in_features  # Get the number of input features to the original fully connected layer\n",
        "model.fc = nn.Linear(num_ftrs, 10)  # Replace the original fc layer with a new one for 10 classes\n"
      ],
      "metadata": {
        "id": "PaZGet0RmoLl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 7: Define the loss function and optimizer\n",
        "# Use CrossEntropyLoss for classification tasks (combines softmax and negative log likelihood loss)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Adam optimizer, but only optimize the final layer (since all other layers are frozen)\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)  # Optimize only the new fully connected layer\n"
      ],
      "metadata": {
        "id": "dfxR53upmoDW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 8: Training loop (simplified)\n",
        "epochs = 5  # Number of epochs (how many times the entire dataset is passed through the model)\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0  # Initialize running loss for each epoch\n",
        "    model.train()  # Set model to training mode (enables layers like dropout, batch norm, etc.)\n",
        "\n",
        "    # Loop over batches of images in the training set\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients from the previous step (avoids accumulation)\n",
        "\n",
        "        # Forward pass: Get model predictions for the inputs\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate the loss between model outputs (predictions) and actual labels\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass: Compute gradients for the trainable parameters (final layer)\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights of the final fully connected layer using the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the loss for monitoring\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss for this epoch\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Print when training is complete\n",
        "print(\"Training Complete.\")\n"
      ],
      "metadata": {
        "id": "WD2FRb63uZFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "\n",
        "\n",
        "1. **Transforms**:\n",
        "   - The **images are resized** to 224x224 pixels, as ResNet models expect input images of this size (the original CIFAR-10 images are 32x32).\n",
        "   - The **images are normalized** to the range `[-1, 1]` using the mean and standard deviation of `(0.5, 0.5, 0.5)` for each channel (R, G, B).\n",
        "\n",
        "2. **Freezing the Convolutional Base**:\n",
        "   - In transfer learning, the convolutional layers of a pre-trained model (in this case, ResNet-18) are used without updating their weights.\n",
        "   - **Freezing** these layers means they won’t be modified during training, which speeds up training and reduces the chance of overfitting.\n",
        "\n",
        "3. **Replacing the Fully Connected Layer**:\n",
        "   - The **original fully connected (fc) layer** of ResNet-18 is designed for 1000 classes (ImageNet). Since CIFAR-10 has only 10 classes, we **replace the fc layer** with a new layer that outputs predictions for 10 classes.\n",
        "\n",
        "4. **Training**:\n",
        "   - The **Adam optimizer** is used to update only the weights of the final fully connected layer, while the rest of the model remains frozen.\n",
        "   - The model is trained over 5 epochs, and the **loss is printed after each epoch** to monitor training progress."
      ],
      "metadata": {
        "id": "uoVblLfT8O8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **6.4. Fine-tuning a Pre-trained Model**\n",
        "- In **fine-tuning**, we allow some or all of the convolutional layers to be retrained, along with the new fully connected layer. This allows the model to adapt its feature extraction capabilities to the new dataset.\n",
        "\n",
        "---\n",
        "\n",
        "**6.4.1. Example: Fine-tuning ResNet18**\n",
        "\n",
        "**Demonstration: Fine-tuning using ResNet18**"
      ],
      "metadata": {
        "id": "_UUUAYUT8O_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained ResNet18 model\n",
        "model = models.resnet18(pretrained=True)  # Load ResNet-18 model pre-trained on ImageNet\n",
        "\n",
        "# Step 1: Unfreeze the last few layers for fine-tuning\n",
        "# We will fine-tune the last residual block (layer4) and the fully connected (fc) layer.\n",
        "for name, param in model.named_parameters():\n",
        "    if \"layer4\" in name or \"fc\" in name:  # Unfreeze parameters of the last residual block (layer4) and the fully connected layer\n",
        "        param.requires_grad = True  # Allow these layers to be updated during training\n",
        "    else:\n",
        "        param.requires_grad = False  # Freeze the rest of the model to prevent weight updates\n",
        "\n",
        "# Step 2: Replace the fully connected (fc) layer to match CIFAR-10's 10 classes\n",
        "# ResNet-18's original fully connected layer is designed for 1000 classes (ImageNet).\n",
        "# We replace it with a new fully connected layer to output predictions for 10 classes (CIFAR-10).\n",
        "num_ftrs = model.fc.in_features  # Get the number of input features to the fully connected layer\n",
        "model.fc = nn.Linear(num_ftrs, 10)  # Replace the fully connected layer with a new one for 10 output classes\n",
        "\n",
        "# Step 3: Define the loss function and optimizer\n",
        "# Use CrossEntropyLoss, which is suitable for classification problems\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Use Adam optimizer, but only optimize the unfrozen layers (layer4 and fc)\n",
        "# 'filter' function is used to pass only the parameters that require gradients to the optimizer\n",
        "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001)  # Lower learning rate for fine-tuning\n",
        "\n",
        "# Step 4: Fine-tuning loop (simplified)\n",
        "epochs = 5  # Number of epochs to fine-tune the model\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0  # Initialize running loss for each epoch\n",
        "    model.train()  # Set model to training mode (enables dropout, batch norm, etc.)\n",
        "\n",
        "    # Iterate over batches of images in the training set\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients to avoid accumulation\n",
        "\n",
        "        # Forward pass: Compute the model outputs for the inputs\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate the loss between model predictions (outputs) and the true labels\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass: Compute gradients for the unfrozen parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the weights of the unfrozen layers (layer4 and fc) using the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate the running loss for monitoring\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss for this epoch\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Print when fine-tuning is complete\n",
        "print(\"Fine-tuning Complete.\")\n"
      ],
      "metadata": {
        "id": "fyv-5sn-8O_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Explanation**:\n",
        "- **Fine-tuning Strategy**: We selectively unfreeze the last block of convolutional layers (`layer4`) and the fully connected layer (`fc`). This allows the model to update its feature extraction for the specific task.\n",
        "- **Lower Learning Rate**: A smaller learning rate (`0.0001`) is used for fine-tuning to avoid large updates that could destroy the pre-trained knowledge.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9XCqUFJv8PBz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **6.5. Observations on Transfer Learning**\n",
        "- **Feature Transferability**: The earlier layers in a CNN tend to capture low-level features (e.g., edges, textures) that are applicable to many tasks, while the later layers capture more task-specific features.\n",
        "- **Efficiency**: Transfer learning significantly reduces the training time and computational resources required compared to training from scratch, especially when working with limited data.\n",
        "- **State-of-the-Art**: Transfer learning is widely used in various applications, including medical imaging, where pre-trained models are adapted to detect diseases from radiological scans, even with small datasets.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PnxWm2n_8PF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **6.6. State-of-the-Art Research on Transfer Learning**\n",
        "- **Pre-trained Models in Natural Language Processing (NLP)**: In NLP, models like **BERT**, **GPT**, and **T5** are pre-trained on massive text corpora and fine-tuned for specific tasks like sentiment analysis or question answering.\n",
        "- **Self-supervised Learning**: Recent advancements in **self-supervised learning** allow models to learn general-purpose features from unlabeled data, which can then be fine-tuned for various downstream tasks.\n",
        "- **Model Scaling**: Research in transfer learning has led to the development of large, scalable models like **EfficientNet** and **BigGAN**, which achieve high performance by balancing the width, depth, and resolution of the models.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ichC1eAi8PJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Continuity to the Next Section\n",
        "- In the next section, we will explore **Recurrent Neural Networks (RNNs)** and **Long Short-Term Memory Networks (LSTMs)**, which are particularly useful for sequential data such as time series, text, and speech.\n",
        "  \n",
        "This section covered the basics of transfer learning, including using pre-trained models for feature extraction and fine-tuning. We leveraged powerful models like ResNet to quickly adapt to new tasks, improving performance with minimal computational effort."
      ],
      "metadata": {
        "id": "c6SBy-6b8PNZ"
      }
    }
  ]
}