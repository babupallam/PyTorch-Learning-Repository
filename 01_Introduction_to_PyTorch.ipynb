{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyErYyyBkD/o7mF6W1B3h2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/PyTorch-Learning-Repository/blob/main/01_Introduction_to_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section 1: Introduction to PyTorch\n",
        "\n",
        "This section will serve as a foundation for learning PyTorch, focusing on tensors, tensor operations, broadcasting, and automatic differentiation. The goal is to ensure a solid understanding of these concepts before moving forward to building neural networks.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "42E2pL3y8NyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **1.1. What is PyTorch?**\n",
        "- PyTorch is an open-source machine learning library developed by Facebook’s AI Research Lab (FAIR).\n",
        "- It is widely used for tasks like deep learning, computer vision, natural language processing, and reinforcement learning.\n",
        "- PyTorch provides flexibility and ease of use with dynamic computation graphs, which allow for more intuitive programming than static computation graphs (like TensorFlow 1.x).\n",
        "- PyTorch supports automatic differentiation, GPU acceleration, and a rich set of APIs for building neural networks.\n"
      ],
      "metadata": {
        "id": "xNZpEnGP-wbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **1.2. PyTorch Installation (On Google Colab)**\n",
        "- Google Colab comes with pre-installed PyTorch, so there’s no need for manual installation.\n",
        "- However, for local environments, you can install PyTorch using the following command:\n",
        "    ```bash\n",
        "    !pip install torch torchvision\n",
        "    ```\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "STGeQLGD-yst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **1.3. Tensors: The Core Data Structure in PyTorch**\n",
        "- **Tensors** are multidimensional arrays that are fundamental to PyTorch (similar to NumPy arrays but optimized for GPUs).\n",
        "- They represent all data inputs, parameters, and outputs in machine learning models.\n",
        "  \n",
        "**Key properties of tensors:**\n",
        "  - Shape: The dimensions of a tensor (e.g., a 2x3 matrix has a shape of `[2, 3]`).\n",
        "  - Dtype: The data type of elements within the tensor (`float32`, `int64`, etc.).\n",
        "  - Device: Tensors can be moved between CPU and GPU using `.to()` or `.cuda()`.\n",
        "\n",
        "---\n",
        "\n",
        "**1.3.1. Basic Tensor Operations**\n",
        "\n",
        "- **Creating Tensors**:\n",
        "  PyTorch provides several functions to create tensors, including `torch.tensor()`, `torch.zeros()`, `torch.ones()`, `torch.randn()`, and more.\n",
        "  \n",
        "  **Demonstration: Basic Tensor Creation**"
      ],
      "metadata": {
        "id": "nMLOdQ_n-0tP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "# Creating a tensor from a list\n",
        "x = torch.tensor([[1, 2], [3, 4]])\n",
        "print(x)  # Output: tensor([[1, 2], [3, 4]])\n",
        "\n",
        "# Creating a tensor filled with zeros\n",
        "zeros_tensor = torch.zeros((2, 3))\n",
        "print(zeros_tensor)  # Output: A 2x3 tensor filled with 0s\n",
        "\n",
        "# Creating a tensor filled with ones\n",
        "ones_tensor = torch.ones((3, 3))\n",
        "print(ones_tensor)  # Output: A 3x3 tensor filled with 1s\n",
        "\n",
        "# Creating a tensor with random values\n",
        "random_tensor = torch.randn((3, 3))\n",
        "print(random_tensor)  # Output: A 3x3 tensor with random values\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCBy22qK8N_n",
        "outputId": "58cf0b9a-4881-4b92-e506-c1f6af2c11fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[ 1.8048, -1.8740,  0.1330],\n",
            "        [ 0.1068,  0.2662, -0.5584],\n",
            "        [-0.0408,  0.6901, -0.8036]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - `torch.tensor()`: Creates a tensor from Python lists or NumPy arrays.\n",
        "  - `torch.zeros()`: Creates a tensor filled with zeros.\n",
        "  - `torch.ones()`: Creates a tensor filled with ones.\n",
        "  - `torch.randn()`: Creates a tensor with random values drawn from a normal distribution.\n",
        "\n",
        "---\n",
        "\n",
        "**1.3.2. Tensor Shapes and Indexing**\n",
        "- Tensor shapes are defined by their dimensions (number of rows, columns, etc.).\n",
        "- PyTorch allows you to easily access and manipulate specific elements, rows, or columns using indexing.\n",
        "\n",
        "  **Demonstration: Tensor Indexing and Shape**"
      ],
      "metadata": {
        "id": "1No9b3ac8O0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape of the tensor\n",
        "print(x.shape)  # Output: torch.Size([2, 2])\n",
        "\n",
        "# Accessing specific elements\n",
        "element = x[1, 0]  # Access element at row 1, column 0\n",
        "print(element)  # Output: tensor(3)\n",
        "\n",
        "# Slicing a tensor\n",
        "slice_tensor = x[:, 1]  # Access all rows, column 1\n",
        "print(slice_tensor)  # Output: tensor([2, 4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlqltPd18O0o",
        "outputId": "b69ccb32-2104-45c0-eb26-912660bf18af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n",
            "tensor(3)\n",
            "tensor([2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - `x.shape`: Returns the shape of the tensor.\n",
        "  - `x[1, 0]`: Indexes the tensor to access the element at row 1, column 0.\n",
        "  - `x[:, 1]`: Slices the tensor to access all rows from column 1.\n",
        "\n",
        "---\n",
        "\n",
        "**1.3.3. Tensor Reshaping and Broadcasting**\n",
        "- **Reshaping** tensors is essential for machine learning, as data often needs to be prepared in a specific format.\n",
        "- **Broadcasting** allows PyTorch to perform operations on tensors with different shapes, as long as the shapes are compatible.\n",
        "\n",
        "  **Demonstration: Tensor Reshaping and Broadcasting**"
      ],
      "metadata": {
        "id": "EpSpCkJJ8O42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Reshaping a tensor\n",
        "reshaped_tensor = x.view(4)\n",
        "print(reshaped_tensor)  # Output: tensor([1, 2, 3, 4])\n",
        "\n",
        "# Broadcasting operation\n",
        "y = torch.tensor([1, 2])\n",
        "broadcast_result = x + y\n",
        "print(broadcast_result)  # Output: tensor([[2, 4], [4, 6]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljWetlMh8O42",
        "outputId": "b8a1f152-ae62-4a8a-b7e0-3487a0f2e6b8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4])\n",
            "tensor([[2, 4],\n",
            "        [4, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - `view()`: Reshapes a tensor without changing its data.\n",
        "  - `x + y`: Broadcasting adds the tensor `y` to each row of tensor `x`.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "uoVblLfT8O8Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **1.4. Operations on Tensors**\n",
        "- PyTorch supports a variety of tensor operations similar to those in NumPy.\n",
        "  \n",
        "**Common Operations:**\n",
        "  - Element-wise operations: `+`, `-`, `*`, `/`, etc.\n",
        "  - Matrix multiplication: `torch.mm()`.\n",
        "  - Aggregation: `sum()`, `mean()`, `max()`, etc.\n",
        "  - Stacking: `torch.cat()`, `torch.stack()`.\n",
        "  - Transpose and permute for changing tensor dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "**1.4.1. Element-wise Operations**\n",
        "\n",
        "  **Demonstration: Element-wise Operations**"
      ],
      "metadata": {
        "id": "cTNdjJrc_Bur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(x)  # Output: tensor([[1, 2], [3, 4]]\n",
        "# Element-wise addition\n",
        "add_result = x + 2\n",
        "print(add_result)  # Output: tensor([[3, 4], [5, 6]])\n",
        "\n",
        "# Element-wise multiplication\n",
        "multiply_result = x * 3\n",
        "print(multiply_result)  # Output: tensor([[ 3,  6], [ 9, 12]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z40mpYvA8O8d",
        "outputId": "74416cc0-9a33-4a0f-a49c-d9c1131a780a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "tensor([[3, 4],\n",
            "        [5, 6]])\n",
            "tensor([[ 3,  6],\n",
            "        [ 9, 12]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - PyTorch supports element-wise operations directly using standard Python operators like `+` and `*`.\n",
        "\n",
        "---\n",
        "\n",
        "**1.4.2. Matrix Multiplication**\n",
        "- Matrix multiplication is a crucial operation in deep learning, used to calculate activations in neural networks.\n",
        "  \n",
        "  **Demonstration: Matrix Multiplication**"
      ],
      "metadata": {
        "id": "_UUUAYUT8O_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Matrix multiplication (dot product)\n",
        "matrix_a = torch.tensor([[1, 2], [3, 4]])\n",
        "matrix_b = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "result = torch.mm(matrix_a, matrix_b)\n",
        "print(result)  # Output: tensor([[19, 22], [43, 50]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyv-5sn-8O_u",
        "outputId": "7fda9286-bbd4-427a-a5ce-f677d982dcb6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[19, 22],\n",
            "        [43, 50]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - `torch.mm()`: Performs matrix multiplication (dot product) between two tensors.\n",
        "  \n",
        "---\n",
        "\n",
        "**1.4.3. Tensor Aggregation**\n",
        "- Aggregation functions are important for tasks like calculating loss or mean squared error in machine learning models.\n",
        "\n",
        "  **Demonstration: Aggregation Functions**"
      ],
      "metadata": {
        "id": "9XCqUFJv8PBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(x)  # Output: tensor([[1, 2], [3, 4]]\n",
        "# Summing all elements in a tensor\n",
        "sum_result = torch.sum(x)\n",
        "print(sum_result)  # Output: tensor(10)\n",
        "\n",
        "# Finding the maximum element\n",
        "max_result = torch.max(x)\n",
        "print(max_result)  # Output: tensor(4)\n",
        "\n",
        "# Calculating the mean\n",
        "mean_result = torch.mean(x.float())\n",
        "print(mean_result)  # Output: tensor(2.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npyM0Iwd8PBz",
        "outputId": "25918071-1d98-47cf-8d6c-64cb41ba2dc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "tensor(10)\n",
            "tensor(4)\n",
            "tensor(2.5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - `torch.sum()`: Sums all elements in the tensor.\n",
        "  - `torch.max()`: Returns the maximum value from the tensor.\n",
        "  - `torch.mean()`: Computes the mean (average) of tensor elements (converted to float if necessary).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PnxWm2n_8PF2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **1.5. Automatic Differentiation (Autograd)**\n",
        "- PyTorch's **autograd** system automates the computation of gradients, which are essential for training neural networks.\n",
        "- **Gradients** are partial derivatives of loss functions with respect to model parameters, helping optimize parameters using gradient descent.\n",
        "  \n",
        "---\n",
        "\n",
        "**1.5.1. Setting Up Autograd**\n",
        "- To enable autograd, tensors should have `requires_grad=True`. This tells PyTorch to track all operations on these tensors.\n",
        "\n",
        "  **Demonstration: Simple Autograd Example**"
      ],
      "metadata": {
        "id": "KGWf6mqX_L4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing torch library for tensor operations and autograd functionality\n",
        "import torch\n",
        "\n",
        "# Step 1: Creating a tensor with requires_grad=True to track computation.\n",
        "# 'requires_grad=True' tells PyTorch that we want to compute gradients with respect to this tensor during backpropagation.\n",
        "# This is necessary when we want to perform automatic differentiation later on.\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# Step 2: Performing an operation on the tensor. Here, we're squaring each element in the tensor 'x'.\n",
        "# PyTorch keeps track of all the operations performed on tensors that have 'requires_grad=True',\n",
        "# so it can later compute gradients for these operations during backpropagation.\n",
        "y = x ** 2\n",
        "\n",
        "# Step 3: Printing the result of the operation.\n",
        "# 'y' is a new tensor that holds the squared values of 'x'. Notice that the tensor has a 'grad_fn' attribute.\n",
        "# This indicates that PyTorch is recording the operation for automatic differentiation. In this case,\n",
        "# 'PowBackward0' is the gradient function created by PyTorch, which will be used during backpropagation.\n",
        "print(y)  # Output: tensor([1., 4., 9.], grad_fn=<PowBackward0>)\n",
        "\n",
        "# Step 4: Summing up the elements of tensor 'y'. This operation will also be tracked for gradient computation.\n",
        "# Treating this sum as a \"loss\" is a common practice in machine learning to reduce a tensor to a scalar.\n",
        "# The 'grad_fn' here is now 'SumBackward0', showing that summing was recorded as well.\n",
        "loss = y.sum()\n",
        "\n",
        "# Step 5: Printing the loss. The loss is a scalar (14.0), which is the sum of [1., 4., 9.].\n",
        "# This will be used for backpropagation in the next step.\n",
        "print(loss)  # Output: tensor(14., grad_fn=<SumBackward0>)\n",
        "\n",
        "# Step 6: Backpropagating the loss to compute gradients. This calculates the derivative of the loss with respect to\n",
        "# each element of 'x'. PyTorch will traverse the recorded operations in reverse (hence \"backward\") to compute the gradients.\n",
        "loss.backward()\n",
        "\n",
        "# Step 7: Accessing the gradient stored in 'x'. After calling 'backward()', the gradients are computed and stored in 'x.grad'.\n",
        "# The gradient is the derivative of the loss (14) with respect to 'x', which in this case turns out to be:\n",
        "# d(loss)/dx = 2 * x (from the derivative of the square operation).\n",
        "# Therefore, x.grad will be [2 * 1.0, 2 * 2.0, 2 * 3.0] = [2.0, 4.0, 6.0].\n",
        "print(x.grad)  # Output: tensor([2., 4., 6.])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJMStppx8PF3",
        "outputId": "01812a35-e87f-44ac-bba2-10ba8215505b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 4., 9.], grad_fn=<PowBackward0>)\n",
            "tensor(14., grad_fn=<SumBackward0>)\n",
            "tensor([2., 4., 6.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - `requires_grad=True`: Tells PyTorch\n",
        "\n",
        " to track gradients for the tensor.\n",
        "  - `.backward()`: Computes the gradient of the loss with respect to the tensor.\n",
        "  - `x.grad`: Holds the computed gradients after backpropagation.\n",
        "\n",
        "---\n",
        "\n",
        "**1.5.2. Observations in Current Research:**\n",
        "  - PyTorch's dynamic computation graph has enabled more flexible model architectures and research prototypes.\n",
        "  - With automatic differentiation, complex models such as transformers, generative models, and reinforcement learning agents can be built rapidly.\n",
        "  - Researchers find PyTorch easier for debugging and iterative experiments compared to static graph-based libraries.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ichC1eAi8PJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### **1.6. Working with GPU Acceleration**\n",
        "- PyTorch makes it easy to move tensors and models to GPU to utilize CUDA for faster computation.\n",
        "\n",
        "  **Demonstration: Using GPU in PyTorch**"
      ],
      "metadata": {
        "id": "UZipaa5o_QDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Checking if a GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)  # Output: 'cuda' if GPU is available, otherwise 'cpu'\n",
        "\n",
        "# Moving a tensor to the GPU\n",
        "x_gpu = x.to(device)\n",
        "print(x_gpu)  # Tensor is now on GPU\n",
        "\n",
        "# Moving it back to CPU\n",
        "x_cpu = x_gpu.to('cpu')\n",
        "print(x_cpu)  # Tensor is now back on CPU\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAuFtD6i8PJr",
        "outputId": "d22f7fa3-72ef-4405-c4a1-6b286b035527"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "tensor([1., 2., 3.], requires_grad=True)\n",
            "tensor([1., 2., 3.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "  **Explanation:**\n",
        "  - `torch.cuda.is_available()`: Checks if a CUDA-enabled GPU is available.\n",
        "  - `.to(device)`: Moves the tensor to the specified device (either CPU or GPU).\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "c6SBy-6b8PNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Continuity to the Next Section:\n",
        "- In the next section, we will explore **building simple neural networks** using PyTorch's `torch.nn` module.\n",
        "- We will leverage the understanding of tensors, operations, and automatic differentiation to implement neural networks from scratch.\n",
        "  "
      ],
      "metadata": {
        "id": "oSJ3t8fK8PQ9"
      }
    }
  ]
}